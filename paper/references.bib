@inproceedings{guo-vosoughi-2023-length,
	title = "Length Does Matter: Summary Length can Bias Summarization Metrics",
	author = "Guo, Xiaobo  and
	Vosoughi, Soroush",
	editor = "Bouamor, Houda  and
	Pino, Juan  and
	Bali, Kalika",
	booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
	month = dec,
	year = "2023",
	address = "Singapore",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.emnlp-main.984",
	doi = "10.18653/v1/2023.emnlp-main.984",
	pages = "15869--15879",
	abstract = "Establishing the characteristics of an effective summary is a complicated and often subjective endeavor. Consequently, the development of metrics for the summarization task has become a dynamic area of research within natural language processing. In this paper, we reveal that existing summarization metrics exhibit a bias toward the length of generated summaries. Our thorough experiments, conducted on a variety of datasets, metrics, and models, substantiate these findings. The results indicate that most metrics tend to favor longer summaries, even after accounting for other factors. To address this issue, we introduce a Bayesian normalization technique that effectively diminishes this bias. We demonstrate that our approach significantly improves the concordance between human annotators and the majority of metrics in terms of summary coherence.",
}


@article{finegrained,
	author       = {Yossi Adi and
	Einat Kermany and
	Yonatan Belinkov and
	Ofer Lavi and
	Yoav Goldberg},
	title        = {Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction
	Tasks},
	journal      = {CoRR},
	volume       = {abs/1608.04207},
	year         = {2016},
	url          = {http://arxiv.org/abs/1608.04207},
	eprinttype    = {arXiv},
	eprint       = {1608.04207},
	timestamp    = {Mon, 13 Aug 2018 16:46:00 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/AdiKBLG16.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{supert,
	author       = {Yang Gao and
	Wei Zhao and
	Steffen Eger},
	title        = {{SUPERT:} Towards New Frontiers in Unsupervised Evaluation Metrics
	for Multi-Document Summarization},
	journal      = {CoRR},
	volume       = {abs/2005.03724},
	year         = {2020},
	url          = {https://arxiv.org/abs/2005.03724},
	eprinttype    = {arXiv},
	eprint       = {2005.03724},
	timestamp    = {Wed, 15 Dec 2021 10:33:30 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2005-03724.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{multirc2,
	author = {Daniel Khashabi and Snigdha Chaturvedi and Michael Roth and Shyam Upadhyay and Dan Roth},
	title = {Looking Beyond the Surface:A Challenge Set for Reading Comprehension over Multiple Sentences},
	booktitle = {Proceedings of North American Chapter of the Association for Computational Linguistics (NAACL)},
	year = {2018}
}
@article{mpnet,
	author       = {Kaitao Song and
	Xu Tan and
	Tao Qin and
	Jianfeng Lu and
	Tie{-}Yan Liu},
	title        = {MPNet: Masked and Permuted Pre-training for Language Understanding},
	journal      = {CoRR},
	volume       = {abs/2004.09297},
	year         = {2020},
	url          = {https://arxiv.org/abs/2004.09297},
	eprinttype    = {arXiv},
	eprint       = {2004.09297},
	timestamp    = {Mon, 29 Jan 2024 17:56:05 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2004-09297.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{5071230,
	author={Liu, Feifan and Liu, Yang},
	journal={IEEE Transactions on Audio, Speech, and Language Processing}, 
	title={Exploring Correlation Between ROUGE and Human Evaluation on Meeting Summaries}, 
	year={2010},
	volume={18},
	number={1},
	pages={187-196},
	keywords={Humans;Speech analysis;Anthropometry;Broadcasting;Computer science;Wire;Character generation;Speech processing;Natural languages;Correlation;disfluencies;evaluation;meeting summarization;ROUGE},
	doi={10.1109/TASL.2009.2025096}}

@ARTICLE{2023arXiv230504853R,
	author = {{Retkowski}, Fabian},
	title = "{The Current State of Summarization}",
	journal = {arXiv e-prints},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	year = 2023,
	month = may,
	eid = {arXiv:2305.04853},
	pages = {arXiv:2305.04853},
	doi = {10.48550/arXiv.2305.04853},
	archivePrefix = {arXiv},
	eprint = {2305.04853},
	primaryClass = {cs.CL},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2023arXiv230504853R},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{hill-etal-2016-learning,
author={Felix Hill, Kyunghyun Cho, and Anna Korhonen},
title={Learning distributed representations of sentences from unlabelled
data},
booktitle={\em Proceedings of the 2016 Conference of the North {A}merican
	Chapter of the Association for Computational Linguistics: Human Language
	Technologies}, 
year={2016}
}

@article{muennighoff2022mteb,
	doi = {10.48550/ARXIV.2210.07316},
	url = {https://arxiv.org/abs/2210.07316},
	author = {Muennighoff, Niklas and Tazi, Nouamane and Magne, Lo{\"\i}c and Reimers, Nils},
	title = {MTEB: Massive Text Embedding Benchmark},
	publisher = {arXiv},
	journal={arXiv preprint arXiv:2210.07316},  
	year = {2022}
}

@article{DBLP:journals/corr/SutskeverVL14,
	author       = {Ilya Sutskever and
	Oriol Vinyals and
	Quoc V. Le},
	title        = {Sequence to Sequence Learning with Neural Networks},
	journal      = {CoRR},
	volume       = {abs/1409.3215},
	year         = {2014},
	url          = {http://arxiv.org/abs/1409.3215},
	eprinttype    = {arXiv},
	eprint       = {1409.3215},
	timestamp    = {Mon, 13 Aug 2018 16:48:06 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/SutskeverVL14.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2002-10957,
	author       = {Wenhui Wang and
	Furu Wei and
	Li Dong and
	Hangbo Bao and
	Nan Yang and
	Ming Zhou},
	title        = {MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression
	of Pre-Trained Transformers},
	journal      = {CoRR},
	volume       = {abs/2002.10957},
	year         = {2020},
	url          = {https://arxiv.org/abs/2002.10957},
	eprinttype    = {arXiv},
	eprint       = {2002.10957},
	timestamp    = {Tue, 03 Mar 2020 14:32:13 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2002-10957.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

{
@misc{Arora2017,
author={Sanjeev Arora, Yingyu Liang, and Tengyu Ma},
title={A simple but tough-to-beat baseline for sentence embeddings.},
booktitle={\em Proceedings of the International Conference on Learning
	Representations}, 
	year={2017}
}

@misc{ rouge,
	author = "Unknown",
	title = "ROUGE Article",
	year = "Unknown" }

@misc{ mead,
	author = "Unknown",
	title = "MEAD Article",
	year = "Unknown" }


@misc{ pyramid,
	author = "Unknown",
	title = "Pyramid Article",
	year = "Unknown" }

@misc{ embedding1,
	author = "Unknown",
	title = "First Embedding Article",
	year = "Unknown" }

@misc{ embedding2,
	author = "Unknown",
	title = "Second Embedding Article",
	year = "Unknown" }

@misc{ randomwalk,
	author = "Krapivsky, P.",
	title = "https://mathoverflow.net/questions/320630/",
	year = "2020" }


@misc{ badembeddings,
	author = "Kennedy, Curtis",
	title = "some-questions-about-text-embedding-ada-002-s-embedding",
	year = "2023" }

@misc{ randomparagraphs,
	author = "Anonymous",
	title = "Random Paragraph Generator: https://randomwordgenerator.com/paragraph.php",
	year = "2023" }

@misc{ tiktoken,
	author = "OpenAI",
	title = "https://github.com/openai/tiktoken",
	year = "2022" }

@misc{ llamatokenizer,
	author = "Unknown",
	title = "Llama Tokenizer",
	year = "Unknown" }

@misc{nikolaev2023investigating,
	title={Investigating semantic subspaces of Transformer sentence embeddings through linear structural probing}, 
	author={Dmitry Nikolaev and Sebastian Padó},
	year={2023},
	eprint={2310.11923},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}


@inproceedings{wang_squality_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {{SQuALITY}: {Building} a {Long}-{Document} {Summarization} {Dataset} the {Hard} {Way}},
	shorttitle = {{SQuALITY}},
	url = {https://aclanthology.org/2022.emnlp-main.75},
	abstract = {Summarization datasets are often assembled either by scraping naturally occurring public-domain summaries—which are nearly always in difficult-to-work-with technical domains—or by using approximate heuristics to extract them from everyday text—which frequently yields unfaithful summaries. In this work, we turn to a slower but more straightforward approach to developing summarization benchmark data: We hire highly-qualified contractors to read stories and write original summaries from scratch. To amortize reading time, we collect five summaries per document, with the first giving an overview and the subsequent four addressing specific questions. We use this protocol to collect SQuALITY, a dataset of question-focused summaries built on the same public-domain short stories as the multiple-choice dataset QuALITY (Pang et al., 2021). Experiments with state-of-the-art summarization systems show that our dataset is challenging and that existing automatic evaluation metrics are weak indicators of quality.},
	urldate = {2023-07-20},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Alex and Pang, Richard Yuanzhe and Chen, Angelica and Phang, Jason and Bowman, Samuel R.},
	month = dec,
	year = {2022},
	pages = {1139--1156},
}

@inproceedings{yuan_bartscore_2021,
	title = {{BARTScore}: {Evaluating} {Generated} {Text} as {Text} {Generation}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/e4d2b6e6fdeca3e60e0f1a62fee3d9dd-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Yuan, Weizhe and Neubig, Graham and Liu, Pengfei},
	editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P. S. and Vaughan, J. Wortman},
	year = {2021},
	pages = {27263--27277},
}

@inproceedings{zhang_bertscore_2019,
	title = {{BERTScore}: {Evaluating} {Text} {Generation} with {BERT}},
	shorttitle = {{BERTScore}},
	url = {https://openreview.net/forum?id=SkeHuCVFDr},
	abstract = {We propose BERTScore, an automatic evaluation metric for text generation. Analogously to common metrics, BERTScore computes a similarity score for each token in the candidate sentence with each token in the reference sentence. However, instead of exact matches, we compute token similarity using contextual embeddings. We evaluate using the outputs of 363 machine translation and image captioning systems. BERTScore correlates better with human judgments and provides stronger model selection performance than existing metrics. Finally, we use an adversarial paraphrase detection task and show that BERTScore is more robust to challenging examples compared to existing metrics.},
	language = {en},
	urldate = {2023-07-27},
	author = {Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q. and Artzi, Yoav},
	month = sep,
	year = {2019},
}

@inproceedings{lin_rouge_2004,
	address = {Barcelona, Spain},
	title = {{ROUGE}: {A} {Package} for {Automatic} {Evaluation} of {Summaries}},
	shorttitle = {{ROUGE}},
	url = {https://aclanthology.org/W04-1013},
	urldate = {2023-07-27},
	booktitle = {Text {Summarization} {Branches} {Out}},
	publisher = {Association for Computational Linguistics},
	author = {Lin, Chin-Yew},
	month = jul,
	year = {2004},
	pages = {74--81},
}

@misc{noauthor_birchai_nodate,
	title = {Birch.{AI} {Automated} summaries for call centers},
	url = {https://birch.ai},
	abstract = {BirchAI leverages foundation models to generate automated summaries and classification of phone calls for complex call center operations in healthcare, finance, insurance and other industries - decreasing average handle time (AHT) by up to 35\%},
	language = {en-US},
	urldate = {2023-07-27},
	journal = {Birch.AI},
}

@misc{larkin_zoom_2022,
	title = {Zoom {IQ} for {Sales}: {Conversational} intelligence for sellers},
	shorttitle = {Zoom {IQ} for {Sales}},
	url = {https://blog.zoom.us/zoom-iq-for-sales/},
	abstract = {Video has revolutionized sales communication, with 67\% of sellers saying it is easier to close deals with their video feed turned on during their},
	language = {en-US},
	urldate = {2023-07-26},
	journal = {Zoom Blog},
	author = {Larkin, Theresa},
	month = apr,
	year = {2022},
}

@inproceedings{wang_toward_2018,
	address = {Brussels, Belgium},
	title = {Toward {Fast} and {Accurate} {Neural} {Discourse} {Segmentation}},
	url = {https://aclanthology.org/D18-1116},
	doi = {10.18653/v1/D18-1116},
	abstract = {Discourse segmentation, which segments texts into Elementary Discourse Units, is a fundamental step in discourse analysis. Previous discourse segmenters rely on complicated hand-crafted features and are not practical in actual use. In this paper, we propose an end-to-end neural segmenter based on BiLSTM-CRF framework. To improve its accuracy, we address the problem of data insufficiency by transferring a word representation model that is trained on a large corpus. We also propose a restricted self-attention mechanism in order to capture useful information within a neighborhood. Experiments on the RST-DT corpus show that our model is significantly faster than previous methods, while achieving new state-of-the-art performance.},
	urldate = {2023-07-24},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Yizhong and Li, Sujian and Yang, Jingfeng},
	month = oct,
	year = {2018},
	pages = {962--967},
}

@article{li_segbot_2018,
	title = {{SegBot}: {A} {Generic} {Neural} {Text} {Segmentation} {Model} with {Pointer} {Network}},
	shorttitle = {{SegBot}},
	url = {https://www.ijcai.org/proceedings/2018/579},
	abstract = {Electronic proceedings of IJCAI 2018},
	urldate = {2023-07-24},
	author = {Li, Jing and Sun, Aixin and Joty, Shafiq},
	year = {2018},
	pages = {4166--4172},
}

@inproceedings{lukasik_text_2020,
	address = {Online},
	title = {Text {Segmentation} by {Cross} {Segment} {Attention}},
	url = {https://aclanthology.org/2020.emnlp-main.380},
	doi = {10.18653/v1/2020.emnlp-main.380},
	abstract = {Document and discourse segmentation are two fundamental NLP tasks pertaining to breaking up text into constituents, which are commonly used to help downstream tasks such as information retrieval or text summarization. In this work, we propose three transformer-based architectures and provide comprehensive comparisons with previously proposed approaches on three standard datasets. We establish a new state-of-the-art, reducing in particular the error rates by a large margin in all cases. We further analyze model sizes and find that we can build models with many fewer parameters while keeping good performance, thus facilitating real-world applications.},
	urldate = {2023-07-24},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Lukasik, Michal and Dadachev, Boris and Papineni, Kishore and Simões, Gonçalo},
	month = nov,
	year = {2020},
	pages = {4707--4716},
}

@inproceedings{koshorek_text_2018,
	address = {New Orleans, Louisiana},
	title = {Text {Segmentation} as a {Supervised} {Learning} {Task}},
	url = {https://aclanthology.org/N18-2075},
	doi = {10.18653/v1/N18-2075},
	abstract = {Text segmentation, the task of dividing a document into contiguous segments based on its semantic structure, is a longstanding challenge in language understanding. Previous work on text segmentation focused on unsupervised methods such as clustering or graph search, due to the paucity in labeled data. In this work, we formulate text segmentation as a supervised learning problem, and present a large new dataset for text segmentation that is automatically extracted and labeled from Wikipedia. Moreover, we develop a segmentation model based on this dataset and show that it generalizes well to unseen natural text.},
	urldate = {2023-07-24},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 2 ({Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Koshorek, Omri and Cohen, Adir and Mor, Noam and Rotman, Michael and Berant, Jonathan},
	month = jun,
	year = {2018},
	pages = {469--473},
}

@article{reed_generalist_2022,
	title = {A {Generalist} {Agent}},
	issn = {2835-8856},
	url = {https://openreview.net/forum?id=1ikK0kHjvj},
	abstract = {Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.},
	language = {en},
	urldate = {2023-07-22},
	journal = {Transactions on Machine Learning Research},
	author = {Reed, Scott and Zolna, Konrad and Parisotto, Emilio and Colmenarejo, Sergio Gómez and Novikov, Alexander and Barth-maron, Gabriel and Giménez, Mai and Sulsky, Yury and Kay, Jackie and Springenberg, Jost Tobias and Eccles, Tom and Bruce, Jake and Razavi, Ali and Edwards, Ashley and Heess, Nicolas and Chen, Yutian and Hadsell, Raia and Vinyals, Oriol and Bordbar, Mahyar and Freitas, Nando de},
	month = aug,
	year = {2022},
}

@inproceedings{hendrycks_measuring_2020,
	title = {Measuring {Massive} {Multitask} {Language} {Understanding}},
	url = {https://openreview.net/forum?id=d7KBjmI3GmQ},
	abstract = {We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.},
	language = {en},
	urldate = {2023-07-22},
	booktitle = {9th {International} {Conference} on {Learning} {Representations}, {ICLR} 2021, {Virtual} {Event}, {Austria}, {May} 3-7, 2021},
	author = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
	month = oct,
	year = {2020},
}

@inproceedings{tay_ul2_2022,
	title = {{UL2}: {Unifying} {Language} {Learning} {Paradigms}},
	shorttitle = {{UL2}},
	url = {https://openreview.net/forum?id=6ruVLB727MC},
	abstract = {Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized and unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5 and/or GPT-like models across multiple diverse setups. Finally, by scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised NLP tasks ranging from language generation (with automated and human evaluation), language understanding, text classification, question answering, commonsense reasoning, long text reasoning, structured knowledge grounding and information retrieval. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. Finally, we show that UL2 20B works well with chain-of-thought prompting and reasoning, making it an appealing choice for research into reasoning at a small to medium scale of 20B parameters. We release Flax-based T5X model checkpoints for the 20B model publicly.},
	language = {en},
	urldate = {2023-07-22},
	booktitle = {The {Eleventh} {International} {Conference} on {Learning} {Representations}, {ICLR} 2023, {Kigali}, {Rwanda}, {May} 1-5, 2023},
	author = {Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q. and Garcia, Xavier and Wei, Jason and Wang, Xuezhi and Chung, Hyung Won and Bahri, Dara and Schuster, Tal and Zheng, Steven and Zhou, Denny and Houlsby, Neil and Metzler, Donald},
	month = sep,
	year = {2022},
}

@inproceedings{liu_generating_2018,
	title = {Generating {Wikipedia} by {Summarizing} {Long} {Sequences}},
	url = {https://openreview.net/forum?id=Hyg0vbWC-},
	abstract = {We show that generating English Wikipedia articles can be approached as a multi- document summarization of source documents. We use extractive summarization to coarsely identify salient information and a neural abstractive model to generate the article. For the abstractive model, we introduce a decoder-only architecture that can scalably attend to very long sequences, much longer than typical encoder- decoder architectures used in sequence transduction. We show that this model can generate fluent, coherent multi-sentence paragraphs and even whole Wikipedia articles. When given reference documents, we show it can extract relevant factual information as reflected in perplexity, ROUGE scores and human evaluations.},
	language = {en},
	urldate = {2023-07-22},
	booktitle = {6th {International} {Conference} on {Learning} {Representations}, {ICLR} 2018,                   {Vancouver}, {BC}, {Canada}, {April} 30 - {May} 3, 2018, {Conference} {Track} {Proceedings}},
	author = {Liu, Peter J. and Saleh, Mohammad and Pot, Etienne and Goodrich, Ben and Sepassi, Ryan and Kaiser, Lukasz and Shazeer, Noam},
	month = feb,
	year = {2018},
}

@inproceedings{jaegle_perceiver_2021,
	title = {Perceiver {IO}: {A} {General} {Architecture} for {Structured} {Inputs} \& {Outputs}},
	shorttitle = {Perceiver {IO}},
	url = {https://openreview.net/forum?id=fILj7WpI-g},
	abstract = {A central goal of machine learning is the development of systems that can solve many problems in as many data domains as possible. Current architectures, however, cannot be applied beyond a small set of stereotyped settings, as they bake in domain \& task assumptions or scale poorly to large inputs or outputs. In this work, we propose Perceiver IO, a general-purpose architecture that handles data from arbitrary settings while scaling linearly with the size of inputs and outputs. Our model augments the Perceiver with a flexible querying mechanism that enables outputs of various sizes and semantics, doing away with the need for task-specific architecture engineering. The same architecture achieves strong results on tasks spanning natural language and visual understanding, multi-task and multi-modal reasoning, and StarCraft II. As highlights, Perceiver IO outperforms a Transformer-based BERT baseline on the GLUE language benchmark despite removing input tokenization and achieves state-of-the-art performance on Sintel optical flow estimation with no explicit mechanisms for multiscale correspondence.},
	language = {en},
	urldate = {2023-07-22},
	booktitle = {The {Tenth} {International} {Conference} on {Learning} {Representations}, {ICLR} 2022, {Virtual} {Event}, {April} 25-29, 2022},
	author = {Jaegle, Andrew and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Doersch, Carl and Ionescu, Catalin and Ding, David and Koppula, Skanda and Zoran, Daniel and Brock, Andrew and Shelhamer, Evan and Henaff, Olivier J. and Botvinick, Matthew and Zisserman, Andrew and Vinyals, Oriol and Carreira, Joao},
	month = oct,
	year = {2021},
}

@article{tay_ul2_2023,
	title = {{UL2}: {UNIFYING} {LANGUAGE} {LEARNING} {PARADIGMS}},
	abstract = {Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a uniﬁed framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives – two concepts that are commonly conﬂated. Next, we present a generalized and uniﬁed perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pretraining objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream ﬁne-tuning is associated with speciﬁc pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and ﬁnd that our method pushes the Pareto-frontier by outperforming T5 and/or GPT-like models across multiple diverse setups. Finally, by scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised NLP tasks ranging from language generation (with automated and human evaluation), language understanding, text classiﬁcation, question answering, commonsense reasoning, long text reasoning, structured knowledge grounding and information retrieval. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on oneshot summarization. Finally, we show that UL2 20B works well with chain-ofthought prompting and reasoning tasks, making it an appealing choice for research into reasoning at a small to medium scale of 20B parameters. We publicly release Flax-based T5X model checkpoints for the 20B model.},
	language = {en},
	author = {Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q and Garcia, Xavier and Wei, Jason and Wang, Xuezhi and Chung, Hyung Won and Bahri, Dara and Schuster, Tal and Zheng, Huaixiu Steven and Zhou, Denny and Houlsby, Neil and Metzler, Donald},
	year = {2023},
}

@inproceedings{feng_survey_2022,
	title = {A {Survey} on {Dialogue} {Summarization}: {Recent} {Advances} and {New} {Frontiers}},
	volume = {6},
	shorttitle = {A {Survey} on {Dialogue} {Summarization}},
	url = {https://www.ijcai.org/proceedings/2022/764},
	doi = {10.24963/ijcai.2022/764},
	abstract = {Electronic proceedings of IJCAI 2022},
	language = {en},
	urldate = {2023-07-21},
	author = {Feng, Xiachong and Feng, Xiaocheng and Qin, Bing},
	month = jul,
	year = {2022},
	note = {ISSN: 1045-0823},
	pages = {5453--5460},
}

@inproceedings{lewis_bart_2020,
	address = {Online},
	title = {{BART}: {Denoising} {Sequence}-to-{Sequence} {Pre}-training for {Natural} {Language} {Generation}, {Translation}, and {Comprehension}},
	shorttitle = {{BART}},
	url = {https://aclanthology.org/2020.acl-main.703},
	doi = {10.18653/v1/2020.acl-main.703},
	abstract = {We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.},
	urldate = {2023-07-21},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Veselin and Zettlemoyer, Luke},
	month = jul,
	year = {2020},
	pages = {7871--7880},
}

@inproceedings{liu_brio_2022,
	address = {Dublin, Ireland},
	title = {{BRIO}: {Bringing} {Order} to {Abstractive} {Summarization}},
	shorttitle = {{BRIO}},
	url = {https://aclanthology.org/2022.acl-long.207},
	doi = {10.18653/v1/2022.acl-long.207},
	abstract = {Abstractive summarization models are commonly trained using maximum likelihood estimation, which assumes a deterministic (one-point) target distribution in which an ideal model will assign all the probability mass to the reference summary. This assumption may lead to performance degradation during inference, where the model needs to compare several system-generated (candidate) summaries that have deviated from the reference summary. To address this problem, we propose a novel training paradigm which assumes a non-deterministic distribution so that different candidate summaries are assigned probability mass according to their quality. Our method achieves a new state-of-the-art result on the CNN/DailyMail (47.78 ROUGE-1) and XSum (49.07 ROUGE-1) datasets. Further analysis also shows that our model can estimate probabilities of candidate summaries that are more correlated with their level of quality.},
	urldate = {2023-07-21},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Liu, Yixin and Liu, Pengfei and Radev, Dragomir and Neubig, Graham},
	month = may,
	year = {2022},
	pages = {2890--2903},
}

@inproceedings{kikuchi_controlling_2016,
	address = {Austin, Texas},
	title = {Controlling {Output} {Length} in {Neural} {Encoder}-{Decoders}},
	url = {https://aclanthology.org/D16-1140},
	doi = {10.18653/v1/D16-1140},
	urldate = {2023-07-21},
	booktitle = {Proceedings of the 2016 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Kikuchi, Yuta and Neubig, Graham and Sasano, Ryohei and Takamura, Hiroya and Okumura, Manabu},
	month = nov,
	year = {2016},
	pages = {1328--1338},
}

@inproceedings{muennighoff_crosslingual_2023,
	address = {Toronto, Canada},
	title = {Crosslingual {Generalization} through {Multitask} {Finetuning}},
	url = {https://aclanthology.org/2023.acl-long.891},
	abstract = {Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task genrealization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at https://github.com/ bigscience-workshop/xmtf.},
	urldate = {2023-07-21},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Muennighoff, Niklas and Wang, Thomas and Sutawika, Lintang and Roberts, Adam and Biderman, Stella and Le Scao, Teven and Bari, M Saiful and Shen, Sheng and Yong, Zheng Xin and Schoelkopf, Hailey and Tang, Xiangru and Radev, Dragomir and Aji, Alham Fikri and Almubarak, Khalid and Albanie, Samuel and Alyafeai, Zaid and Webson, Albert and Raff, Edward and Raffel, Colin},
	month = jul,
	year = {2023},
	pages = {15991--16111},
}

@inproceedings{he_ctrlsum_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {{CTRLsum}: {Towards} {Generic} {Controllable} {Text} {Summarization}},
	shorttitle = {{CTRLsum}},
	url = {https://aclanthology.org/2022.emnlp-main.396},
	abstract = {Current summarization systems yield generic summaries that are disconnected from users' preferences and expectations. To address this limitation, we present CTRLsum, a generic framework to control generated summaries through a set of keywords. During training keywords are extracted automatically without requiring additional human annotations. At test time CTRLsum features a control function to map control signal to keywords; through engineering the control function, the same trained model is able to be applied to control summaries on various dimensions, while neither affecting the model training process nor the pretrained models. We additionally explore the combination of keywords and text prompts for more control tasks. Experiments demonstrate the effectiveness of CTRLsum on three domains of summarization datasets and five control tasks: (1) entity-centric and (2) length-controllable summarization, (3) contribution summarization on scientific papers, (4) invention purpose summarization on patent filings, and (5) question-guided summarization on news articles. Moreover, when used in a standard, unconstrained summarization setting, CTRLsum is comparable or better than strong pretrained systems.},
	urldate = {2023-07-21},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {He, Junxian and Kryscinski, Wojciech and McCann, Bryan and Rajani, Nazneen and Xiong, Caiming},
	month = dec,
	year = {2022},
	pages = {5879--5915},
}

@inproceedings{christiano_deep_2017,
	title = {Deep {Reinforcement} {Learning} from {Human} {Preferences}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
	editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
}

@inproceedings{karpukhin_dense_2020,
	address = {Online},
	title = {Dense {Passage} {Retrieval} for {Open}-{Domain} {Question} {Answering}},
	url = {https://aclanthology.org/2020.emnlp-main.550},
	doi = {10.18653/v1/2020.emnlp-main.550},
	abstract = {Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system greatly by 9\%-19\% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.},
	urldate = {2023-07-21},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Karpukhin, Vladimir and Oguz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
	month = nov,
	year = {2020},
	pages = {6769--6781},
}

@inproceedings{kryscinski_evaluating_2020,
	address = {Online},
	title = {Evaluating the {Factual} {Consistency} of {Abstractive} {Text} {Summarization}},
	url = {https://aclanthology.org/2020.emnlp-main.750},
	doi = {10.18653/v1/2020.emnlp-main.750},
	abstract = {The most common metrics for assessing summarization algorithms do not account for whether summaries are factually consistent with source documents. We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between source documents and generated summaries. Training data is generated by applying a series of rule-based transformations to the sentences of source documents.The factual consistency model is then trained jointly for three tasks: 1) predict whether each summary sentence is factually consistent or not, 2) in either case, extract a span in the source document to support this consistency prediction, 3) for each summary sentence that is deemed inconsistent, extract the inconsistent span from it. Transferring this model to summaries generated by several neural models reveals that this highly scalable approach outperforms previous models, including those trained with strong supervision using datasets from related domains, such as natural language inference and fact checking. Additionally, human evaluation shows that the auxiliary span extraction tasks provide useful assistance in the process of verifying factual consistency. We also release a manually annotated dataset for factual consistency verification, code for training data generation, and trained model weights at https://github.com/salesforce/factCC.},
	urldate = {2023-07-20},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Kryscinski, Wojciech and McCann, Bryan and Xiong, Caiming and Socher, Richard},
	month = nov,
	year = {2020},
	pages = {9332--9346},
}

@inproceedings{ladhak_exploring_2020,
	address = {Online},
	title = {Exploring {Content} {Selection} in {Summarization} of {Novel} {Chapters}},
	url = {https://aclanthology.org/2020.acl-main.453},
	doi = {10.18653/v1/2020.acl-main.453},
	abstract = {We present a new summarization task, generating summaries of novel chapters using summary/chapter pairs from online study guides. This is a harder task than the news summarization task, given the chapter length as well as the extreme paraphrasing and generalization found in the summaries. We focus on extractive summarization, which requires the creation of a gold-standard set of extractive summaries. We present a new metric for aligning reference summary sentences with chapter sentences to create gold extracts and also experiment with different alignment methods. Our experiments demonstrate significant improvement over prior alignment approaches for our task as shown through automatic metrics and a crowd-sourced pyramid analysis.},
	urldate = {2023-07-20},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Ladhak, Faisal and Li, Bryan and Al-Onaizan, Yaser and McKeown, Kathleen},
	month = jul,
	year = {2020},
	pages = {5043--5054},
}

@article{raffel_exploring_2020,
	title = {Exploring the {Limits} of {Transfer} {Learning} with a {Unified} {Text}-to-{Text} {Transformer}},
	volume = {21},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v21/20-074.html},
	abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new “Colossal Clean Crawled Corpus”, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
	number = {140},
	urldate = {2023-07-20},
	journal = {Journal of Machine Learning Research},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	year = {2020},
	pages = {1--67},
}

@inproceedings{wan_faithfulness-aware_2023,
	address = {Dubrovnik, Croatia},
	title = {Faithfulness-{Aware} {Decoding} {Strategies} for {Abstractive} {Summarization}},
	url = {https://aclanthology.org/2023.eacl-main.210},
	abstract = {Despite significant progress in understanding and improving faithfulness in abstractive summarization, the question of how decoding strategies affect faithfulness is less studied. We present a systematic study of the effect of generation techniques such as beam search and nucleus sampling on faithfulness in abstractive summarization. We find a consistent trend where beam search with large beam sizes produces the most faithful summaries while nucleus sampling generates the least faithful ones. We propose two faithfulness-aware generation methods to further improve faithfulness over current generation techniques: (1) ranking candidates generated by beam search using automatic faithfulness metrics and (2) incorporating lookahead heuristics that produce a faithfulness score on the future summary. We show that both generation methods significantly improve faithfulness across two datasets as evaluated by four automatic faithfulness metrics and human evaluation. To reduce computational cost, we demonstrate a simple distillation approach that allows the model to generate faithful summaries with just greedy decoding.},
	urldate = {2023-07-20},
	booktitle = {Proceedings of the 17th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Wan, David and Liu, Mengwen and McKeown, Kathleen and Dreyer, Markus and Bansal, Mohit},
	month = may,
	year = {2023},
	pages = {2864--2880},
}

@inproceedings{see_get_2017,
	address = {Vancouver, Canada},
	title = {Get {To} {The} {Point}: {Summarization} with {Pointer}-{Generator} {Networks}},
	shorttitle = {Get {To} {The} {Point}},
	url = {https://aclanthology.org/P17-1099},
	doi = {10.18653/v1/P17-1099},
	abstract = {Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.},
	urldate = {2023-07-20},
	booktitle = {Proceedings of the 55th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {See, Abigail and Liu, Peter J. and Manning, Christopher D.},
	month = jul,
	year = {2017},
	pages = {1073--1083},
}

@inproceedings{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {1877--1901},
}

@inproceedings{verma_large_2023,
	address = {Dubrovnik, Croatia},
	title = {Large {Scale} {Multi}-{Lingual} {Multi}-{Modal} {Summarization} {Dataset}},
	url = {https://aclanthology.org/2023.eacl-main.263},
	abstract = {Significant developments in techniques such as encoder-decoder models have enabled us to represent information comprising multiple modalities. This information can further enhance many downstream tasks in the field of information retrieval and natural language processing; however, improvements in multi-modal techniques and their performance evaluation require large-scale multi-modal data which offers sufficient diversity. Multi-lingual modeling for a variety of tasks like multi-modal summarization, text generation, and translation leverages information derived from high-quality multi-lingual annotated data. In this work, we present the current largest multi-lingual multi-modal summarization dataset (M3LS), and it consists of over a million instances of document-image pairs along with a professionally annotated multi-modal summary for each pair. It is derived from news articles published by British Broadcasting Corporation(BBC) over a decade and spans 20 languages, targeting diversity across five language roots, it is also the largest summarization dataset for 13 languages and consists of cross-lingual summarization data for 2 languages. We formally define the multi-lingual multi-modal summarization task utilizing our dataset and report baseline scores from various state-of-the-art summarization techniques in a multi-lingual setting. We also compare it with many similar datasets to analyze the uniqueness and difficulty of M3LS. The dataset and code used in this work are made available at “https://github.com/anubhav-jangra/M3LS”.},
	urldate = {2023-07-20},
	booktitle = {Proceedings of the 17th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Verma, Yash and Jangra, Anubhav and Verma, Raghvendra and Saha, Sriparna},
	month = may,
	year = {2023},
	pages = {3620--3632},
}

@misc{mikolov2013efficient,
title={Efficient Estimation of Word Representations in Vector Space}, 
author={Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
year={2013},
eprint={1301.3781},
archivePrefix={arXiv},
primaryClass={cs.CL}
}

@article{DBLP:journals/corr/abs-1907-11692,
author       = {Yinhan Liu and
Myle Ott and
Naman Goyal and
Jingfei Du and
Mandar Joshi and
Danqi Chen and
Omer Levy and
Mike Lewis and
Luke Zettlemoyer and
Veselin Stoyanov},
title        = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
journal      = {CoRR},
volume       = {abs/1907.11692},
year         = {2019},
url          = {http://arxiv.org/abs/1907.11692},
eprinttype    = {arXiv},
eprint       = {1907.11692},
timestamp    = {Thu, 14 Dec 2023 18:03:41 +0100},
biburl       = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1810-04805,
author       = {Jacob Devlin and
Ming{-}Wei Chang and
Kenton Lee and
Kristina Toutanova},
title        = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
Understanding},
journal      = {CoRR},
volume       = {abs/1810.04805},
year         = {2018},
url          = {http://arxiv.org/abs/1810.04805},
eprinttype    = {arXiv},
eprint       = {1810.04805},
timestamp    = {Tue, 30 Oct 2018 20:39:56 +0100},
biburl       = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{pennington-etal-2014-glove,
title = "{G}lo{V}e: Global Vectors for Word Representation",
author = "Pennington, Jeffrey  and
Socher, Richard  and
Manning, Christopher",
editor = "Moschitti, Alessandro  and
Pang, Bo  and
Daelemans, Walter",
booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
month = oct,
year = "2014",
address = "Doha, Qatar",
publisher = "Association for Computational Linguistics",
url = "https://aclanthology.org/D14-1162",
doi = "10.3115/v1/D14-1162",
pages = "1532--1543",
}


@inproceedings{guo_longt5_2022,
	address = {Seattle, United States},
	title = {{LongT5}: {Efficient} {Text}-{To}-{Text} {Transformer} for {Long} {Sequences}},
	shorttitle = {{LongT5}},
	url = {https://aclanthology.org/2022.findings-naacl.55},
	doi = {10.18653/v1/2022.findings-naacl.55},
	abstract = {Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the performance of Transformer-based neural models. In this paper, we present LongT5, a new model that explores the effects of scaling both the input length and model size at the same time. Specifically, we integrate attention ideas from long-input transformers (ETC), and adopt pre-training strategies from summarization pre-training (PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call Transient Global (TGlobal), which mimics ETC's local/global attention mechanism, but without requiring additional side-inputs. We are able to achieve state-of-the-art results on several summarization and question answering tasks, as well as outperform the original T5 models on these tasks. We have open sourced our architecture and training code, as well as our pre-trained model checkpoints.},
	urldate = {2023-07-20},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {NAACL} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Guo, Mandy and Ainslie, Joshua and Uthus, David and Ontanon, Santiago and Ni, Jianmo and Sung, Yun-Hsuan and Yang, Yinfei},
	month = jul,
	year = {2022},
	pages = {724--736},
}

@inproceedings{zhang_pegasus_2020,
	title = {{PEGASUS}: {Pre}-training with {Extracted} {Gap}-sentences for {Abstractive} {Summarization}},
	shorttitle = {{PEGASUS}},
	url = {https://proceedings.mlr.press/v119/zhang20ae.html},
	abstract = {Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new self-supervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets.},
	language = {en},
	urldate = {2023-07-20},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {11328--11339},
}

@inproceedings{jaegle_perceiver_2021-1,
	title = {Perceiver {IO}: {A} {General} {Architecture} for {Structured} {Inputs} \& {Outputs}},
	shorttitle = {Perceiver {IO}},
	url = {https://openreview.net/forum?id=fILj7WpI-g},
	abstract = {A central goal of machine learning is the development of systems that can solve many problems in as many data domains as possible. Current architectures, however, cannot be applied beyond a small set of stereotyped settings, as they bake in domain \& task assumptions or scale poorly to large inputs or outputs. In this work, we propose Perceiver IO, a general-purpose architecture that handles data from arbitrary settings while scaling linearly with the size of inputs and outputs. Our model augments the Perceiver with a flexible querying mechanism that enables outputs of various sizes and semantics, doing away with the need for task-specific architecture engineering. The same architecture achieves strong results on tasks spanning natural language and visual understanding, multi-task and multi-modal reasoning, and StarCraft II. As highlights, Perceiver IO outperforms a Transformer-based BERT baseline on the GLUE language benchmark despite removing input tokenization and achieves state-of-the-art performance on Sintel optical flow estimation with no explicit mechanisms for multiscale correspondence.},
	language = {en},
	urldate = {2023-07-20},
	author = {Jaegle, Andrew and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Doersch, Carl and Ionescu, Catalin and Ding, David and Koppula, Skanda and Zoran, Daniel and Brock, Andrew and Shelhamer, Evan and Henaff, Olivier J. and Botvinick, Matthew and Zisserman, Andrew and Vinyals, Oriol and Carreira, Joao},
	month = oct,
	year = {2021},
}

@inproceedings{xiao_primera_2022,
	address = {Dublin, Ireland},
	title = {{PRIMERA}: {Pyramid}-based {Masked} {Sentence} {Pre}-training for {Multi}-document {Summarization}},
	shorttitle = {{PRIMERA}},
	url = {https://aclanthology.org/2022.acl-long.360},
	doi = {10.18653/v1/2022.acl-long.360},
	abstract = {We introduce PRIMERA, a pre-trained model for multi-document representation with a focus on summarization that reduces the need for dataset-specific architectures and large amounts of fine-tuning labeled data. PRIMERA uses our newly proposed pre-training objective designed to teach the model to connect and aggregate information across documents. It also uses efficient encoder-decoder transformers to simplify the processing of concatenated input documents. With extensive experiments on 6 multi-document summarization datasets from 3 different domains on zero-shot, few-shot and full-supervised settings, PRIMERA outperforms current state-of-the-art dataset-specific and pre-trained models on most of these settings with large margins.},
	urldate = {2023-07-20},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Xiao, Wen and Beltagy, Iz and Carenini, Giuseppe and Cohan, Arman},
	month = may,
	year = {2022},
	pages = {5245--5263},
}

@article{gehrmann_repairing_2023,
	title = {Repairing the {Cracked} {Foundation}: {A} {Survey} of {Obstacles} in {Evaluation} {Practices} for {Generated} {Text}},
	volume = {77},
	copyright = {Copyright (c) 2023 Journal of Artificial Intelligence Research},
	issn = {1076-9757},
	shorttitle = {Repairing the {Cracked} {Foundation}},
	url = {https://www.jair.org/index.php/jair/article/view/13715},
	doi = {10.1613/jair.1.13715},
	abstract = {Evaluation practices in natural language generation (NLG) have many known flaws, but improved evaluation approaches are rarely widely adopted. This issue has become more urgent, since neural generation models have improved to the point where their outputs can often no longer be distinguished based on the surface-level features that older metrics rely on. This paper surveys the issues with human and automatic model evaluations and with commonly used datasets in NLG that have been pointed out over the past 20 years. We summarize, categorize, and discuss how researchers have been addressing these issues and what their findings mean for the current state of model evaluations. Building on those insights, we lay out a long-term vision for evaluation research and propose concrete steps for researchers to improve their evaluation processes. Finally, we analyze 66 generation papers from recent NLP conferences in how well they already follow these suggestions and identify which areas require more drastic changes to the status quo.},
	language = {en},
	urldate = {2023-07-20},
	journal = {Journal of Artificial Intelligence Research},
	author = {Gehrmann, Sebastian and Clark, Elizabeth and Sellam, Thibault},
	month = may,
	year = {2023},
	keywords = {human computer interaction, natural language, neural networks},
	pages = {103--166},
}

@inproceedings{liu_revisiting_2023,
	address = {Toronto, Canada},
	title = {Revisiting the {Gold} {Standard}: {Grounding} {Summarization} {Evaluation} with {Robust} {Human} {Evaluation}},
	shorttitle = {Revisiting the {Gold} {Standard}},
	url = {https://aclanthology.org/2023.acl-long.228},
	abstract = {Human evaluation is the foundation upon which the evaluation of both summarization systems and automatic metrics rests. However, existing human evaluation studies for summarization either exhibit a low inter-annotator agreement or have insufficient scale, and an in-depth analysis of human evaluation is lacking. Therefore, we address the shortcomings of existing summarization evaluation along the following axes: (1) We propose a modified summarization salience protocol, Atomic Content Units (ACUs), which is based on fine-grained semantic units and allows for a high inter-annotator agreement. (2) We curate the Robust Summarization Evaluation (RoSE) benchmark, a large human evaluation dataset consisting of 22,000 summary-level annotations over 28 top-performing systems on three datasets. (3) We conduct a comparative study of four human evaluation protocols, underscoring potential confounding factors in evaluation setups. (4) We evaluate 50 automatic metrics and their variants using the collected human annotations across evaluation protocols and demonstrate how our benchmark leads to more statistically stable and significant results. The metrics we benchmarked include recent methods based on large language models (LLMs), GPTScore and G-Eval. Furthermore, our findings have important implications for evaluating LLMs, as we show that LLMs adjusted by human feedback (e.g., GPT-3.5) may overfit unconstrained human evaluation, which is affected by the annotators' prior, input-agnostic preferences, calling for more robust, targeted evaluation methods.},
	urldate = {2023-07-20},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Liu, Yixin and Fabbri, Alex and Liu, Pengfei and Zhao, Yilun and Nan, Linyong and Han, Ruilin and Han, Simeng and Joty, Shafiq and Wu, Chien-Sheng and Xiong, Caiming and Radev, Dragomir},
	month = jul,
	year = {2023},
	pages = {4140--4170},
}

@inproceedings{zhong_searching_2019,
	address = {Florence, Italy},
	title = {Searching for {Effective} {Neural} {Extractive} {Summarization}: {What} {Works} and {What}'s {Next}},
	shorttitle = {Searching for {Effective} {Neural} {Extractive} {Summarization}},
	url = {https://aclanthology.org/P19-1100},
	doi = {10.18653/v1/P19-1100},
	abstract = {The recent years have seen remarkable success in the use of deep neural networks on text summarization. However, there is no clear understanding of why they perform so well, or how they might be improved. In this paper, we seek to better understand how neural extractive summarization systems could benefit from different types of model architectures, transferable knowledge and learning schemas. Besides, we find an effective way to improve the current framework and achieve the state-of-the-art result on CNN/DailyMail by a large margin based on our observations and analysis. Hopefully, our work could provide more hints for future research on extractive summarization.},
	urldate = {2023-07-20},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Zhong, Ming and Liu, Pengfei and Wang, Danqing and Qiu, Xipeng and Huang, Xuanjing},
	month = jul,
	year = {2019},
	pages = {1049--1058},
}

@inproceedings{wang_self-instruct_2023,
	address = {Toronto, Canada},
	title = {Self-{Instruct}: {Aligning} {Language} {Models} with {Self}-{Generated} {Instructions}},
	shorttitle = {Self-{Instruct}},
	url = {https://aclanthology.org/2023.acl-long.754},
	abstract = {Large “instruction-tuned” language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33\% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5\% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.},
	urldate = {2023-07-20},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A. and Khashabi, Daniel and Hajishirzi, Hannaneh},
	month = jul,
	year = {2023},
	pages = {13484--13508},
}

@inproceedings{pagnoni_socratic_2023,
	address = {Toronto, Canada},
	title = {Socratic {Pretraining}: {Question}-{Driven} {Pretraining} for {Controllable} {Summarization}},
	shorttitle = {Socratic {Pretraining}},
	url = {https://aclanthology.org/2023.acl-long.713},
	abstract = {In long document controllable summarization, where labeled data is scarce, pretrained models struggle to adapt to the task and effectively respond to user queries. In this paper, we introduce Socratic pretraining, a question-driven, unsupervised pretraining objective specifically designed to improve controllability in summarization tasks. By training a model to generate and answer relevant questions in a given context, Socratic pretraining enables the model to more effectively adhere to user-provided queries and identify relevant content to be summarized. We demonstrate the effectiveness of this approach through extensive experimentation on two summarization domains, short stories and dialogue, and multiple control strategies: keywords, questions, and factoid QA pairs. Our pretraining method relies only on unlabeled documents and a question generation system and outperforms pre-finetuning approaches that use additional supervised data. Furthermore, our results show that Socratic pretraining cuts task-specific labeled data requirements in half, is more faithful to user-provided queries, and achieves state-of-the-art performance on QMSum and SQuALITY.},
	urldate = {2023-07-20},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Pagnoni, Artidoro and Fabbri, Alex and Kryscinski, Wojciech and Wu, Chien-Sheng},
	month = jul,
	year = {2023},
	pages = {12737--12755},
}

@inproceedings{zhang_summn_2022,
	address = {Dublin, Ireland},
	title = {Summ{\textasciicircum}{N}: {A} {Multi}-{Stage} {Summarization} {Framework} for {Long} {Input} {Dialogues} and {Documents}},
	shorttitle = {Summ{\textasciicircum}{N}},
	url = {https://aclanthology.org/2022.acl-long.112},
	doi = {10.18653/v1/2022.acl-long.112},
	abstract = {Text summarization helps readers capture salient information from documents, news, interviews, and meetings. However, most state-of-the-art pretrained language models (LM) are unable to efficiently process long text for many summarization tasks. In this paper, we propose Summ{\textasciicircum}N, a simple, flexible, and effective multi-stage framework for input texts that are longer than the maximum context length of typical pretrained LMs. Summ{\textasciicircum}N first splits the data samples and generates a coarse summary in multiple stages and then produces the final fine-grained summary based on it. Our framework can process input text of arbitrary length by adjusting the number of stages while keeping the LM input size fixed. Moreover, it can deal with both single-source documents and dialogues, and it can be used on top of different backbone abstractive summarization models. To the best of our knowledge, Summ{\textasciicircum}N is the first multi-stage split-then-summarize framework for long input summarization. Our experiments demonstrate that Summ{\textasciicircum}N outperforms previous state-of-the-art methods by improving ROUGE scores on three long meeting summarization datasets AMI, ICSI, and QMSum, two long TV series datasets from SummScreen, and a long document summarization dataset GovReport. Our data and code are available at https://github.com/psunlpgroup/Summ-N.},
	urldate = {2023-07-20},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Yusen and Ni, Ansong and Mao, Ziming and Wu, Chen Henry and Zhu, Chenguang and Deb, Budhaditya and Awadallah, Ahmed and Radev, Dragomir and Zhang, Rui},
	month = may,
	year = {2022},
	pages = {1592--1604},
}

@article{fabbri_summeval_2021,
	title = {{SummEval}: {Re}-evaluating {Summarization} {Evaluation}},
	volume = {9},
	issn = {2307-387X},
	shorttitle = {{SummEval}},
	url = {https://doi.org/10.1162/tacl_a_00373},
	doi = {10.1162/tacl_a_00373},
	abstract = {The scarcity of comprehensive up-to-date studies on evaluation metrics for text summarization and the lack of consensus regarding evaluation protocols continue to inhibit progress. We address the existing shortcomings of summarization evaluation methods along five dimensions: 1) we re-evaluate 14 automatic evaluation metrics in a comprehensive and consistent fashion using neural summarization model outputs along with expert and crowd-sourced human annotations; 2) we consistently benchmark 23 recent summarization models using the aforementioned automatic evaluation metrics; 3) we assemble the largest collection of summaries generated by models trained on the CNN/DailyMail news dataset and share it in a unified format; 4) we implement and share a toolkit that provides an extensible and unified API for evaluating summarization models across a broad range of automatic metrics; and 5) we assemble and share the largest and most diverse, in terms of model types, collection of human judgments of model-generated summaries on the CNN/Daily Mail dataset annotated by both expert judges and crowd-source workers. We hope that this work will help promote a more complete evaluation protocol for text summarization as well as advance research in developing evaluation metrics that better correlate with human judgments.},
	urldate = {2023-07-20},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Fabbri, Alexander R. and Kryściński, Wojciech and McCann, Bryan and Xiong, Caiming and Socher, Richard and Radev, Dragomir},
	month = apr,
	year = {2021},
	pages = {391--409},
}

@article{fabbri_summeval_2021-1,
	title = {{SummEval}: {Re}-evaluating {Summarization} {Evaluation}},
	volume = {9},
	issn = {2307-387X},
	shorttitle = {{SummEval}},
	url = {https://doi.org/10.1162/tacl_a_00373},
	doi = {10.1162/tacl_a_00373},
	abstract = {The scarcity of comprehensive up-to-date studies on evaluation metrics for text summarization and the lack of consensus regarding evaluation protocols continue to inhibit progress. We address the existing shortcomings of summarization evaluation methods along five dimensions: 1) we re-evaluate 14 automatic evaluation metrics in a comprehensive and consistent fashion using neural summarization model outputs along with expert and crowd-sourced human annotations; 2) we consistently benchmark 23 recent summarization models using the aforementioned automatic evaluation metrics; 3) we assemble the largest collection of summaries generated by models trained on the CNN/DailyMail news dataset and share it in a unified format; 4) we implement and share a toolkit that provides an extensible and unified API for evaluating summarization models across a broad range of automatic metrics; and 5) we assemble and share the largest and most diverse, in terms of model types, collection of human judgments of model-generated summaries on the CNN/Daily Mail dataset annotated by both expert judges and crowd-source workers. We hope that this work will help promote a more complete evaluation protocol for text summarization as well as advance research in developing evaluation metrics that better correlate with human judgments.},
	urldate = {2023-07-20},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Fabbri, Alexander R. and Kryściński, Wojciech and McCann, Bryan and Xiong, Caiming and Socher, Richard and Radev, Dragomir},
	month = apr,
	year = {2021},
	pages = {391--409},
}

@inproceedings{wang_superglue_2019,
	title = {{SuperGLUE}: {A} {Stickier} {Benchmark} for {General}-{Purpose} {Language} {Understanding} {Systems}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/4496bf24afe7fab6f046bf4923da8de6-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d' and Fox, E. and Garnett, R.},
	year = {2019},
}

@inproceedings{liu_text_2019,
	address = {Hong Kong, China},
	title = {Text {Summarization} with {Pretrained} {Encoders}},
	url = {https://aclanthology.org/D19-1387},
	doi = {10.18653/v1/D19-1387},
	abstract = {Bidirectional Encoder Representations from Transformers (BERT) represents the latest incarnation of pretrained language models which have recently advanced a wide range of natural language processing tasks. In this paper, we showcase how BERT can be usefully applied in text summarization and propose a general framework for both extractive and abstractive models. We introduce a novel document-level encoder based on BERT which is able to express the semantics of a document and obtain representations for its sentences. Our extractive model is built on top of this encoder by stacking several inter-sentence Transformer layers. For abstractive summarization, we propose a new fine-tuning schedule which adopts different optimizers for the encoder and the decoder as a means of alleviating the mismatch between the two (the former is pretrained while the latter is not). We also demonstrate that a two-staged fine-tuning approach can further boost the quality of the generated summaries. Experiments on three datasets show that our model achieves state-of-the-art results across the board in both extractive and abstractive settings.},
	urldate = {2023-07-20},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Liu, Yang and Lapata, Mirella},
	month = nov,
	year = {2019},
	pages = {3730--3740},
}

@inproceedings{cachola_tldr_2020,
	address = {Online},
	title = {{TLDR}: {Extreme} {Summarization} of {Scientific} {Documents}},
	shorttitle = {{TLDR}},
	url = {https://aclanthology.org/2020.findings-emnlp.428},
	doi = {10.18653/v1/2020.findings-emnlp.428},
	abstract = {We introduce TLDR generation, a new form of extreme summarization, for scientific papers. TLDR generation involves high source compression and requires expert background knowledge and understanding of complex domain-specific language. To facilitate study on this task, we introduce SCITLDR, a new multi-target dataset of 5.4K TLDRs over 3.2K papers. SCITLDR contains both author-written and expert-derived TLDRs, where the latter are collected using a novel annotation protocol that produces high-quality summaries while minimizing annotation burden. We propose CATTS, a simple yet effective learning strategy for generating TLDRs that exploits titles as an auxiliary training signal. CATTS improves upon strong baselines under both automated metrics and human evaluations. Data and code are publicly available at https://github.com/allenai/scitldr.},
	urldate = {2023-07-20},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Cachola, Isabel and Lo, Kyle and Cohan, Arman and Weld, Daniel},
	month = nov,
	year = {2020},
	pages = {4766--4777},
}

@inproceedings{ouyang_training_2022,
	title = {Training language models to follow instructions with human feedback},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul F and Leike, Jan and Lowe, Ryan},
	editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
	year = {2022},
	pages = {27730--27744},
}

@misc{noauthor_training_nodate,
	title = {Training language models to follow instructions with human feedback},
	url = {https://proceedings.neurips.cc//paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html},
	urldate = {2023-07-20},
}

@article{tay_ul2_2023-1,
	title = {{UL2}: {UNIFYING} {LANGUAGE} {LEARNING} {PARADIGMS}},
	abstract = {Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a uniﬁed framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives – two concepts that are commonly conﬂated. Next, we present a generalized and uniﬁed perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pretraining objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream ﬁne-tuning is associated with speciﬁc pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and ﬁnd that our method pushes the Pareto-frontier by outperforming T5 and/or GPT-like models across multiple diverse setups. Finally, by scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised NLP tasks ranging from language generation (with automated and human evaluation), language understanding, text classiﬁcation, question answering, commonsense reasoning, long text reasoning, structured knowledge grounding and information retrieval. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on oneshot summarization. Finally, we show that UL2 20B works well with chain-ofthought prompting and reasoning tasks, making it an appealing choice for research into reasoning at a small to medium scale of 20B parameters. We publicly release Flax-based T5X model checkpoints for the 20B model.},
	language = {en},
	author = {Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q and Garcia, Xavier and Wei, Jason and Wang, Xuezhi and Chung, Hyung Won and Bahri, Dara and Schuster, Tal and Zheng, Huaixiu Steven and Zhou, Denny and Houlsby, Neil and Metzler, Donald},
	year = {2023},
}

@inproceedings{pagnoni_understanding_2021,
	address = {Online},
	title = {Understanding {Factuality} in {Abstractive} {Summarization} with {FRANK}: {A} {Benchmark} for {Factuality} {Metrics}},
	shorttitle = {Understanding {Factuality} in {Abstractive} {Summarization} with {FRANK}},
	url = {https://aclanthology.org/2021.naacl-main.383},
	doi = {10.18653/v1/2021.naacl-main.383},
	abstract = {Modern summarization models generate highly fluent but often factually unreliable outputs. This motivated a surge of metrics attempting to measure the factuality of automatically generated summaries. Due to the lack of common benchmarks, these metrics cannot be compared. Moreover, all these methods treat factuality as a binary concept and fail to provide deeper insights on the kinds of inconsistencies made by different systems. To address these limitations, we devise a typology of factual errors and use it to collect human annotations of generated summaries from state-of-the-art summarization systems for the CNN/DM and XSum datasets. Through these annotations we identify the proportion of different categories of factual errors and benchmark factuality metrics, showing their correlation with human judgement as well as their specific strengths and weaknesses.},
	urldate = {2023-07-20},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Pagnoni, Artidoro and Balachandran, Vidhisha and Tsvetkov, Yulia},
	month = jun,
	year = {2021},
	pages = {4812--4829},
}

@inproceedings{he_z-code_2023,
	address = {Toronto, Canada},
	title = {Z-{Code}++: {A} {Pre}-trained {Language} {Model} {Optimized} for {Abstractive} {Summarization}},
	shorttitle = {Z-{Code}++},
	url = {https://aclanthology.org/2023.acl-long.279},
	abstract = {This paper presents Z-Code++, a new pre-trained language model optimized for abstractive text summarization. The model extends the state-of-the-art encoder-decoder model using three techniques. First, we use a two-phase pre-training to improve the model's performance on low-resource summarization tasks. The model is first pre-trained using text corpora for language understanding, then is continually pre-trained on summarization corpora for grounded text generation. Second, we replace self-attention layers in the encoder with disentangled attention layers, where each word is represented using two vectors that encode its content and position, respectively. Third, we use fusion-in-encoder, a simple yet effective method of encoding long sequences in a hierarchical manner. Z-Code++ createsa new state-of-the-art on 9 of 13 text summarization tasks across 5 languages. Our model is parameter-efficient in that it outperforms the 600x larger PaLM540B on XSum, and the finetuned 200x larger GPT3175B on SAMSum. In zero-shot and few-shot settings, our model substantially outperforms the competing models.},
	urldate = {2023-07-20},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {He, Pengcheng and Peng, Baolin and Wang, Song and Liu, Yang and Xu, Ruochen and Hassan, Hany and Shi, Yu and Zhu, Chenguang and Xiong, Wayne and Zeng, Michael and Gao, Jianfeng and Huang, Xuedong},
	month = jul,
	year = {2023},
	pages = {5095--5112},
}

@inproceedings{bhaskar_prompted_2023,
	address = {Toronto, Canada},
	title = {Prompted {Opinion} {Summarization} with {GPT}-3.5},
	url = {https://aclanthology.org/2023.findings-acl.591},
	abstract = {Large language models have shown impressive performance across a wide variety of tasks, including text summarization. In this paper, we show that this strong performance extends to opinion summarization. We explore several pipeline methods for applying GPT-3.5 to summarize a large collection of user reviews in aprompted fashion. To handle arbitrarily large numbers of user reviews, we explore recursive summarization as well as methods for selecting salient content to summarize through supervised clustering or extraction. On two datasets, an aspect-oriented summarization dataset of hotel reviews (SPACE) and a generic summarization dataset of Amazon and Yelp reviews (FewSum), we show that GPT-3.5 models achieve very strong performance in human evaluation. We argue that standard evaluation metrics do not reflect this, and introduce three new metrics targeting faithfulness, factuality, and genericity to contrast these different methods.},
	urldate = {2023-07-20},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Bhaskar, Adithya and Fabbri, Alex and Durrett, Greg},
	month = jul,
	year = {2023},
	pages = {9282--9300},
}

@misc{bhaskar_prompted_2023-1,
	title = {Prompted {Opinion} {Summarization} with {GPT}-3.5},
	url = {http://arxiv.org/abs/2211.15914},
	doi = {10.48550/arXiv.2211.15914},
	abstract = {Large language models have shown impressive performance across a wide variety of tasks, including text summarization. In this paper, we show that this strong performance extends to opinion summarization. We explore several pipeline methods for applying GPT-3.5 to summarize a large collection of user reviews in a prompted fashion. To handle arbitrarily large numbers of user reviews, we explore recursive summarization as well as methods for selecting salient content to summarize through supervised clustering or extraction. On two datasets, an aspect-oriented summarization dataset of hotel reviews (SPACE) and a generic summarization dataset of Amazon and Yelp reviews (FewSum), we show that GPT-3.5 models achieve very strong performance in human evaluation. We argue that standard evaluation metrics do not reflect this, and introduce three new metrics targeting faithfulness, factuality, and genericity to contrast these different methods.},
	urldate = {2023-07-20},
	publisher = {arXiv},
	author = {Bhaskar, Adithya and Fabbri, Alexander R. and Durrett, Greg},
	month = may,
	year = {2023},
	note = {arXiv:2211.15914 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{gekhman_trueteacher_2023,
	title = {{TrueTeacher}: {Learning} {Factual} {Consistency} {Evaluation} with {Large} {Language} {Models}},
	shorttitle = {{TrueTeacher}},
	url = {http://arxiv.org/abs/2305.11171},
	doi = {10.48550/arXiv.2305.11171},
	abstract = {Factual consistency evaluation is often conducted using Natural Language Inference (NLI) models, yet these models exhibit limited success in evaluating summaries. Previous work improved such models with synthetic training data. However, the data is typically based on perturbed human-written summaries, which often differ in their characteristics from real model-generated summaries and have limited coverage of possible factual errors. Alternatively, large language models (LLMs) have recently shown promising results in directly evaluating generative tasks, but are too computationally expensive for practical use. Motivated by these limitations, we introduce TrueTeacher, a method for generating synthetic data by annotating diverse model-generated summaries using a LLM. Unlike prior work, TrueTeacher does not rely on human-written summaries, and is multilingual by nature. Experiments on the TRUE benchmark show that a student model trained using our data, substantially outperforms both the state-of-the-art model with similar capacity, and the LLM teacher. In a systematic study, we compare TrueTeacher to existing synthetic data generation methods and demonstrate its superiority and robustness to domain-shift. Using the the mFACE dataset, we also show that our method generalizes to multilingual scenarios. Finally, we release a large-scale synthetic dataset with 1.4M examples generated using TrueTeacher.},
	urldate = {2023-05-19},
	publisher = {arXiv},
	author = {Gekhman, Zorik and Herzig, Jonathan and Aharoni, Roee and Elkind, Chen and Szpektor, Idan},
	month = may,
	year = {2023},
	note = {arXiv:2305.11171 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{noauthor_ak_2023,
	title = {{AK} on {Twitter}},
	url = {https://twitter.com/_akhaliq/status/1659014411976077312},
	abstract = {“CoEdIT: Text Editing by Task-Specific Instruction Tuning
	
	paper page: https://t.co/EFSgV011Rl”},
	language = {en},
	urldate = {2023-05-19},
	journal = {Twitter},
	month = may,
	year = {2023},
}

@misc{saunders_self-critiquing_2022,
	title = {Self-critiquing models for assisting human evaluators},
	url = {http://arxiv.org/abs/2206.05802},
	doi = {10.48550/arXiv.2206.05802},
	abstract = {We fine-tune large language models to write natural language critiques (natural language critical comments) using behavioral cloning. On a topic-based summarization task, critiques written by our models help humans find flaws in summaries that they would have otherwise missed. Our models help find naturally occurring flaws in both model and human written summaries, and intentional flaws in summaries written by humans to be deliberately misleading. We study scaling properties of critiquing with both topic-based summarization and synthetic tasks. Larger models write more helpful critiques, and on most tasks, are better at self-critiquing, despite having harder-to-critique outputs. Larger models can also integrate their own self-critiques as feedback, refining their own summaries into better ones. Finally, we motivate and introduce a framework for comparing critiquing ability to generation and discrimination ability. Our measurements suggest that even large models may still have relevant knowledge they cannot or do not articulate as critiques. These results are a proof of concept for using AI-assisted human feedback to scale the supervision of machine learning systems to tasks that are difficult for humans to evaluate directly. We release our training datasets, as well as samples from our critique assistance experiments.},
	urldate = {2023-05-19},
	publisher = {arXiv},
	author = {Saunders, William and Yeh, Catherine and Wu, Jeff and Bills, Steven and Ouyang, Long and Ward, Jonathan and Leike, Jan},
	month = jun,
	year = {2022},
	note = {arXiv:2206.05802 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{raffel_exploring_2019,
	title = {Exploring the {Limits} of {Transfer} {Learning} with a {Unified} {Text}-to-{Text} {Transformer}},
	url = {https://arxiv.org/abs/1910.10683v3},
	doi = {10.48550/arXiv.1910.10683},
	abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
	language = {en},
	urldate = {2023-02-28},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	month = oct,
	year = {2019},
}

@misc{liu_brio_2022-1,
	title = {{BRIO}: {Bringing} {Order} to {Abstractive} {Summarization}},
	shorttitle = {{BRIO}},
	url = {https://arxiv.org/abs/2203.16804v1},
	doi = {10.48550/arXiv.2203.16804},
	abstract = {Abstractive summarization models are commonly trained using maximum likelihood estimation, which assumes a deterministic (one-point) target distribution in which an ideal model will assign all the probability mass to the reference summary. This assumption may lead to performance degradation during inference, where the model needs to compare several system-generated (candidate) summaries that have deviated from the reference summary. To address this problem, we propose a novel training paradigm which assumes a non-deterministic distribution so that different candidate summaries are assigned probability mass according to their quality. Our method achieves a new state-of-the-art result on the CNN/DailyMail (47.78 ROUGE-1) and XSum (49.07 ROUGE-1) datasets. Further analysis also shows that our model can estimate probabilities of candidate summaries that are more correlated with their level of quality.},
	language = {en},
	urldate = {2023-02-28},
	author = {Liu, Yixin and Liu, Pengfei and Radev, Dragomir and Neubig, Graham},
	month = mar,
	year = {2022},
}

@misc{lewis_bart_2019,
	title = {{BART}: {Denoising} {Sequence}-to-{Sequence} {Pre}-training for {Natural} {Language} {Generation}, {Translation}, and {Comprehension}},
	shorttitle = {{BART}},
	url = {https://arxiv.org/abs/1910.13461v1},
	doi = {10.48550/arXiv.1910.13461},
	abstract = {We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance.},
	language = {en},
	urldate = {2023-02-28},
	author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
	month = oct,
	year = {2019},
}

@misc{zhang_pegasus_2019,
	title = {{PEGASUS}: {Pre}-training with {Extracted} {Gap}-sentences for {Abstractive} {Summarization}},
	shorttitle = {{PEGASUS}},
	url = {https://arxiv.org/abs/1912.08777v3},
	doi = {10.48550/arXiv.1912.08777},
	language = {en},
	urldate = {2023-02-28},
	author = {Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter J.},
	month = dec,
	year = {2019},
}

@misc{gao_human-like_2023,
	title = {Human-like {Summarization} {Evaluation} with {ChatGPT}},
	url = {http://arxiv.org/abs/2304.02554},
	abstract = {Evaluating text summarization is a challenging problem, and existing evaluation metrics are far from satisfactory. In this study, we explored ChatGPT's ability to perform human-like summarization evaluation using four human evaluation methods on five datasets. We found that ChatGPT was able to complete annotations relatively smoothly using Likert scale scoring, pairwise comparison, Pyramid, and binary factuality evaluation. Additionally, it outperformed commonly used automatic evaluation metrics on some datasets. Furthermore, we discussed the impact of different prompts, compared its performance with that of human evaluation, and analyzed the generated explanations and invalid responses.},
	urldate = {2023-05-08},
	publisher = {arXiv},
	author = {Gao, Mingqi and Ruan, Jie and Sun, Renliang and Yin, Xunjian and Yang, Shiping and Wan, Xiaojun},
	month = apr,
	year = {2023},
	note = {arXiv:2304.02554 [cs]
	version: 1},
	keywords = {Computer Science - Computation and Language},
}

@misc{zhang_extractive_2023,
	title = {Extractive {Summarization} via {ChatGPT} for {Faithful} {Summary} {Generation}},
	url = {http://arxiv.org/abs/2304.04193},
	abstract = {Extractive summarization is a crucial task in natural language processing that aims to condense long documents into shorter versions by directly extracting sentences. The recent introduction of ChatGPT has attracted significant interest in the NLP community due to its remarkable performance on a wide range of downstream tasks. However, concerns regarding factuality and faithfulness have hindered its practical applications for summarization systems. This paper first presents a thorough evaluation of ChatGPT's performance on extractive summarization and compares it with traditional fine-tuning methods on various benchmark datasets. Our experimental analysis reveals that ChatGPT's extractive summarization performance is still inferior to existing supervised systems in terms of ROUGE scores. In addition, we explore the effectiveness of in-context learning and chain-of-thought reasoning for enhancing its performance. Furthermore, we find that applying an extract-then-generate pipeline with ChatGPT yields significant performance improvements over abstractive baselines in terms of summary faithfulness. These observations highlight potential directions for enhancing ChatGPT's capabilities for faithful text summarization tasks using two-stage approaches.},
	urldate = {2023-05-08},
	publisher = {arXiv},
	author = {Zhang, Haopeng and Liu, Xiao and Zhang, Jiawei},
	month = apr,
	year = {2023},
	note = {arXiv:2304.04193 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{xiao_chatgpt-steered_2023,
	title = {{ChatGPT}-steered {Editing} {Instructor} for {Customization} of {Abstractive} {Summarization}},
	url = {http://arxiv.org/abs/2305.02483},
	abstract = {Tailoring outputs of large language models, such as ChatGPT, to specific user needs remains a challenge despite their impressive generation quality. In this paper, we propose a tri-agent generation pipeline consisting of a generator, an instructor, and an editor to enhance the customization of generated outputs. The generator produces an initial output, the user-specific instructor generates editing instructions, and the editor generates a revised output aligned with user preferences. The inference-only large language model (ChatGPT) serves as both the generator and the editor, while a smaller model acts as the user-specific instructor to guide the generation process toward user needs. The instructor is trained using editor-steered reinforcement learning, leveraging feedback from the large-scale editor model to optimize instruction generation. Experimental results on two abstractive summarization datasets demonstrate the effectiveness of our approach in generating outputs that better fulfill user expectations.},
	urldate = {2023-05-08},
	publisher = {arXiv},
	author = {Xiao, Wen and Xie, Yujia and Carenini, Giuseppe and He, Pengcheng},
	month = may,
	year = {2023},
	note = {arXiv:2305.02483 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{fu_gptscore_2023,
	title = {{GPTScore}: {Evaluate} as {You} {Desire}},
	shorttitle = {{GPTScore}},
	url = {http://arxiv.org/abs/2302.04166},
	abstract = {Generative Artificial Intelligence (AI) has enabled the development of sophisticated models that are capable of producing high-caliber text, images, and other outputs through the utilization of large pre-trained models. Nevertheless, assessing the quality of the generation is an even more arduous task than the generation itself, and this issue has not been given adequate consideration recently. This paper proposes a novel evaluation framework, GPTScore, which utilizes the emergent abilities (e.g., zero-shot instruction) of generative pre-trained models to score generated texts. There are 19 pre-trained models explored in this paper, ranging in size from 80M (e.g., FLAN-T5-small) to 175B (e.g., GPT3). Experimental results on four text generation tasks, 22 evaluation aspects, and corresponding 37 datasets demonstrate that this approach can effectively allow us to achieve what one desires to evaluate for texts simply by natural language instructions. This nature helps us overcome several long-standing challenges in text evaluation--how to achieve customized, multi-faceted evaluation without the need for annotated samples. We make our code publicly available at https://github.com/jinlanfu/GPTScore.},
	urldate = {2023-05-08},
	publisher = {arXiv},
	author = {Fu, Jinlan and Ng, See-Kiong and Jiang, Zhengbao and Liu, Pengfei},
	month = feb,
	year = {2023},
	note = {arXiv:2302.04166 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{cardenas_trade-off_2022,
	title = {On the {Trade}-off between {Redundancy} and {Local} {Coherence} in {Summarization}},
	url = {http://arxiv.org/abs/2205.10192},
	abstract = {Extractive summarization systems are known to produce poorly coherent and, if not accounted for, highly redundant text. In this work, we tackle the problem of summary redundancy in unsupervised extractive summarization of long, highly-redundant documents. For this, we leverage a psycholinguistic theory of human reading comprehension which directly models local coherence and redundancy. Implementing this theory, our system operates at the proposition level and exploits properties of human memory representations to rank similarly content units that are coherent and non-redundant, hence encouraging the extraction of less redundant final summaries. Because of the impact of the summary length on automatic measures, we control for it by formulating content selection as an optimization problem with soft constraints in the budget of information retrieved. Using summarization of scientific articles as a case study, extensive experiments demonstrate that the proposed systems extract consistently less redundant summaries across increasing levels of document redundancy, whilst maintaining comparable performance (in terms of relevancy and local coherence) against strong unsupervised baselines according to automated evaluations.},
	urldate = {2023-05-08},
	publisher = {arXiv},
	author = {Cardenas, Ronald and Galle, Matthias and Cohen, Shay B.},
	month = may,
	year = {2022},
	note = {arXiv:2205.10192 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{taori_alpaca_2023,
	title = {Alpaca: {A} {Strong}, {Replicable} {Instruction}-{Following} {Model}},
	url = {https://crfm.stanford.edu/2023/03/13/alpaca.html},
	urldate = {2023-05-07},
	journal = {Stanford CRFM},
	author = {Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B.},
	month = mar,
	year = {2023},
}

@misc{the_vicuna_team_vicuna_2023,
	title = {Vicuna: {An} {Open}-{Source} {Chatbot} {Impressing} {GPT}-4 with 90\%* {ChatGPT} {Quality} {\textbar} {LMSYS} {Org}},
	shorttitle = {Vicuna},
	url = {https://lmsys.org/blog/2023-03-30-vicuna},
	language = {en},
	urldate = {2023-05-07},
	author = {{The Vicuna Team}},
	month = mar,
	year = {2023},
}

@misc{wang_self-instruct_2022,
	title = {Self-{Instruct}: {Aligning} {Language} {Model} with {Self} {Generated} {Instructions}},
	shorttitle = {Self-{Instruct}},
	url = {http://arxiv.org/abs/2212.10560},
	abstract = {Large "instruction-tuned" language models (finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off its own generations. Our pipeline generates instruction, input, and output samples from a language model, then prunes them before using them to finetune the original model. Applying our method to vanilla GPT3, we demonstrate a 33\% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT\_001, which is trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5\% absolute gap behind InstructGPT\_001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.},
	urldate = {2023-05-07},
	publisher = {arXiv},
	author = {Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A. and Khashabi, Daniel and Hajishirzi, Hannaneh},
	month = dec,
	year = {2022},
	note = {arXiv:2212.10560 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{goodwin_flight_2020,
	address = {Barcelona, Spain (Online)},
	title = {Flight of the {PEGASUS}? {Comparing} {Transformers} on {Few}-shot and {Zero}-shot {Multi}-document {Abstractive} {Summarization}},
	shorttitle = {Flight of the {PEGASUS}?},
	url = {https://aclanthology.org/2020.coling-main.494},
	doi = {10.18653/v1/2020.coling-main.494},
	abstract = {Recent work has shown that pre-trained Transformers obtain remarkable performance on many natural language processing tasks including automatic summarization. However, most work has focused on (relatively) data-rich single-document summarization settings. In this paper, we explore highly-abstractive multi-document summarization where the summary is explicitly conditioned on a user-given topic statement or question. We compare the summarization quality produced by three state-of-the-art transformer-based models: BART, T5, and PEGASUS. We report the performance on four challenging summarization datasets: three from the general domain and one from consumer health in both zero-shot and few-shot learning settings. While prior work has shown significant differences in performance for these models on standard summarization tasks, our results indicate that with as few as 10 labeled examples there is no statistically significant difference in summary quality, suggesting the need for more abstractive benchmark collections when determining state-of-the-art.},
	urldate = {2023-05-07},
	booktitle = {Proceedings of the 28th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {International Committee on Computational Linguistics},
	author = {Goodwin, Travis and Savery, Max and Demner-Fushman, Dina},
	month = dec,
	year = {2020},
	pages = {5640--5646},
}

@inproceedings{li_leveraging_2020,
	address = {Online},
	title = {Leveraging {Graph} to {Improve} {Abstractive} {Multi}-{Document} {Summarization}},
	url = {https://aclanthology.org/2020.acl-main.555},
	doi = {10.18653/v1/2020.acl-main.555},
	abstract = {Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries. Our model utilizes graphs to encode documents in order to capture cross-document relations, which is crucial to summarizing long documents. Our model can also take advantage of graphs to guide the summary generation process, which is beneficial for generating coherent and concise summaries. Furthermore, pre-trained language models can be easily combined with our model, which further improve the summarization performance significantly. Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines.},
	urldate = {2023-05-06},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Li, Wei and Xiao, Xinyan and Liu, Jiachen and Wu, Hua and Wang, Haifeng and Du, Junping},
	month = jul,
	year = {2020},
	pages = {6232--6243},
}

@inproceedings{liu_hierarchical_2019,
	address = {Florence, Italy},
	title = {Hierarchical {Transformers} for {Multi}-{Document} {Summarization}},
	url = {https://aclanthology.org/P19-1500},
	doi = {10.18653/v1/P19-1500},
	abstract = {In this paper, we develop a neural summarization model which can effectively process multiple input documents and distill Transformer architecture with the ability to encode documents in a hierarchical manner. We represent cross-document relationships via an attention mechanism which allows to share information as opposed to simply concatenating text spans and processing them as a flat sequence. Our model learns latent dependencies among textual units, but can also take advantage of explicit graph representations focusing on similarity or discourse relations. Empirical results on the WikiSum dataset demonstrate that the proposed architecture brings substantial improvements over several strong baselines.},
	urldate = {2023-05-06},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Liu, Yang and Lapata, Mirella},
	month = jul,
	year = {2019},
	pages = {5070--5081},
}

@misc{xiao_primera_2022-1,
	title = {{PRIMERA}: {Pyramid}-based {Masked} {Sentence} {Pre}-training for {Multi}-document {Summarization}},
	shorttitle = {{PRIMERA}},
	url = {http://arxiv.org/abs/2110.08499},
	abstract = {We introduce PRIMERA, a pre-trained model for multi-document representation with a focus on summarization that reduces the need for dataset-specific architectures and large amounts of fine-tuning labeled data. PRIMERA uses our newly proposed pre-training objective designed to teach the model to connect and aggregate information across documents. It also uses efficient encoder-decoder transformers to simplify the processing of concatenated input documents. With extensive experiments on 6 multi-document summarization datasets from 3 different domains on zero-shot, few-shot and full-supervised settings, PRIMERA outperforms current state-of-the-art dataset-specific and pre-trained models on most of these settings with large margins. The code and pre-trained models can be found at {\textbackslash}url\{https://github.com/allenai/PRIMER\}.},
	urldate = {2023-05-06},
	publisher = {arXiv},
	author = {Xiao, Wen and Beltagy, Iz and Carenini, Giuseppe and Cohan, Arman},
	month = mar,
	year = {2022},
	note = {arXiv:2110.08499 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{jin_multi-granularity_2020,
	address = {Online},
	title = {Multi-{Granularity} {Interaction} {Network} for {Extractive} and {Abstractive} {Multi}-{Document} {Summarization}},
	url = {https://aclanthology.org/2020.acl-main.556},
	doi = {10.18653/v1/2020.acl-main.556},
	abstract = {In this paper, we propose a multi-granularity interaction network for extractive and abstractive multi-document summarization, which jointly learn semantic representations for words, sentences, and documents. The word representations are used to generate an abstractive summary while the sentence representations are used to produce an extractive summary. We employ attention mechanisms to interact between different granularity of semantic representations, which helps to capture multi-granularity key information and improves the performance of both abstractive and extractive summarization. Experiment results show that our proposed model substantially outperforms all strong baseline methods and achieves the best results on the Multi-News dataset.},
	urldate = {2023-05-06},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Jin, Hanqi and Wang, Tianming and Wan, Xiaojun},
	month = jul,
	year = {2020},
	pages = {6244--6254},
}

@inproceedings{pasunuru_efficiently_2021,
	address = {Online},
	title = {Efficiently {Summarizing} {Text} and {Graph} {Encodings} of {Multi}-{Document} {Clusters}},
	url = {https://aclanthology.org/2021.naacl-main.380},
	doi = {10.18653/v1/2021.naacl-main.380},
	abstract = {This paper presents an efficient graph-enhanced approach to multi-document summarization (MDS) with an encoder-decoder Transformer model. This model is based on recent advances in pre-training both encoder and decoder on very large text data (Lewis et al., 2019), and it incorporates an efficient encoding mechanism (Beltagy et al., 2020) that avoids the quadratic memory growth typical for traditional Transformers. We show that this powerful combination not only scales to large input documents commonly found when summarizing news clusters; it also enables us to process additional input in the form of auxiliary graph representations, which we derive from the multi-document clusters. We present a mechanism to incorporate such graph information into the encoder-decoder model that was pre-trained on text only. Our approach leads to significant improvements on the Multi-News dataset, overall leading to an average 1.8 ROUGE score improvement over previous work (Li et al., 2020). We also show improvements in a transfer-only setup on the DUC-2004 dataset. The graph encodings lead to summaries that are more abstractive. Human evaluation shows that they are also more informative and factually more consistent with their input documents.},
	urldate = {2023-05-06},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Pasunuru, Ramakanth and Liu, Mengwen and Bansal, Mohit and Ravi, Sujith and Dreyer, Markus},
	month = jun,
	year = {2021},
	pages = {4768--4779},
}

@misc{liu_generating_2018-1,
	title = {Generating {Wikipedia} by {Summarizing} {Long} {Sequences}},
	url = {http://arxiv.org/abs/1801.10198},
	abstract = {We show that generating English Wikipedia articles can be approached as a multi- document summarization of source documents. We use extractive summarization to coarsely identify salient information and a neural abstractive model to generate the article. For the abstractive model, we introduce a decoder-only architecture that can scalably attend to very long sequences, much longer than typical encoder- decoder architectures used in sequence transduction. We show that this model can generate fluent, coherent multi-sentence paragraphs and even whole Wikipedia articles. When given reference documents, we show it can extract relevant factual information as reflected in perplexity, ROUGE scores and human evaluations.},
	urldate = {2023-05-06},
	publisher = {arXiv},
	author = {Liu, Peter J. and Saleh, Mohammad and Pot, Etienne and Goodrich, Ben and Sepassi, Ryan and Kaiser, Lukasz and Shazeer, Noam},
	month = jan,
	year = {2018},
	note = {arXiv:1801.10198 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{li_multi-modal_2017,
	address = {Copenhagen, Denmark},
	title = {Multi-modal {Summarization} for {Asynchronous} {Collection} of {Text}, {Image}, {Audio} and {Video}},
	url = {https://aclanthology.org/D17-1114},
	doi = {10.18653/v1/D17-1114},
	abstract = {The rapid increase of the multimedia data over the Internet necessitates multi-modal summarization from collections of text, image, audio and video. In this work, we propose an extractive Multi-modal Summarization (MMS) method which can automatically generate a textual summary given a set of documents, images, audios and videos related to a specific topic. The key idea is to bridge the semantic gaps between multi-modal contents. For audio information, we design an approach to selectively use its transcription. For vision information, we learn joint representations of texts and images using a neural network. Finally, all the multi-modal aspects are considered to generate the textural summary by maximizing the salience, non-redundancy, readability and coverage through budgeted optimization of submodular functions. We further introduce an MMS corpus in English and Chinese. The experimental results on this dataset demonstrate that our method outperforms other competitive baseline methods.},
	urldate = {2023-05-06},
	booktitle = {Proceedings of the 2017 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Li, Haoran and Zhu, Junnan and Ma, Cong and Zhang, Jiajun and Zong, Chengqing},
	month = sep,
	year = {2017},
	pages = {1092--1102},
}

@article{reed_generalist_2022-1,
	title = {A {Generalist} {Agent}},
	issn = {2835-8856},
	url = {https://openreview.net/forum?id=1ikK0kHjvj},
	abstract = {Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.},
	language = {en},
	urldate = {2023-05-01},
	journal = {Transactions on Machine Learning Research},
	author = {Reed, Scott and Zolna, Konrad and Parisotto, Emilio and Colmenarejo, Sergio Gómez and Novikov, Alexander and Barth-maron, Gabriel and Giménez, Mai and Sulsky, Yury and Kay, Jackie and Springenberg, Jost Tobias and Eccles, Tom and Bruce, Jake and Razavi, Ali and Edwards, Ashley and Heess, Nicolas and Chen, Yutian and Hadsell, Raia and Vinyals, Oriol and Bordbar, Mahyar and Freitas, Nando de},
	month = nov,
	year = {2022},
}

@misc{jaegle_perceiver_2022,
	title = {Perceiver {IO}: {A} {General} {Architecture} for {Structured} {Inputs} \& {Outputs}},
	shorttitle = {Perceiver {IO}},
	url = {http://arxiv.org/abs/2107.14795},
	doi = {10.48550/arXiv.2107.14795},
	abstract = {A central goal of machine learning is the development of systems that can solve many problems in as many data domains as possible. Current architectures, however, cannot be applied beyond a small set of stereotyped settings, as they bake in domain \& task assumptions or scale poorly to large inputs or outputs. In this work, we propose Perceiver IO, a general-purpose architecture that handles data from arbitrary settings while scaling linearly with the size of inputs and outputs. Our model augments the Perceiver with a flexible querying mechanism that enables outputs of various sizes and semantics, doing away with the need for task-specific architecture engineering. The same architecture achieves strong results on tasks spanning natural language and visual understanding, multi-task and multi-modal reasoning, and StarCraft II. As highlights, Perceiver IO outperforms a Transformer-based BERT baseline on the GLUE language benchmark despite removing input tokenization and achieves state-of-the-art performance on Sintel optical flow estimation with no explicit mechanisms for multiscale correspondence.},
	urldate = {2023-05-01},
	publisher = {arXiv},
	author = {Jaegle, Andrew and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Doersch, Carl and Ionescu, Catalin and Ding, David and Koppula, Skanda and Zoran, Daniel and Brock, Andrew and Shelhamer, Evan and Hénaff, Olivier and Botvinick, Matthew M. and Zisserman, Andrew and Vinyals, Oriol and Carreira, Joāo},
	month = mar,
	year = {2022},
	note = {arXiv:2107.14795 [cs, eess]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{jangra_survey_2023,
	title = {A {Survey} on {Multi}-modal {Summarization}},
	url = {http://arxiv.org/abs/2109.05199},
	abstract = {The new era of technology has brought us to the point where it is convenient for people to share their opinions over an abundance of platforms. These platforms have a provision for the users to express themselves in multiple forms of representations, including text, images, videos, and audio. This, however, makes it difficult for users to obtain all the key information about a topic, making the task of automatic multi-modal summarization (MMS) essential. In this paper, we present a comprehensive survey of the existing research in the area of MMS, covering various modalities like text, image, audio, and video. Apart from highlighting the different evaluation metrics and datasets used for the MMS task, our work also discusses the current challenges and future directions in this field.},
	urldate = {2023-05-01},
	publisher = {arXiv},
	author = {Jangra, Anubhav and Mukherjee, Sourajit and Jatowt, Adam and Saha, Sriparna and Hasanuzzaman, Mohammad},
	month = feb,
	year = {2023},
	note = {arXiv:2109.05199 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Multimedia, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{li_keep_2019,
	address = {Florence, Italy},
	title = {Keep {Meeting} {Summaries} on {Topic}: {Abstractive} {Multi}-{Modal} {Meeting} {Summarization}},
	shorttitle = {Keep {Meeting} {Summaries} on {Topic}},
	url = {https://aclanthology.org/P19-1210},
	doi = {10.18653/v1/P19-1210},
	abstract = {Transcripts of natural, multi-person meetings differ significantly from documents like news articles, which can make Natural Language Generation models for generating summaries unfocused. We develop an abstractive meeting summarizer from both videos and audios of meeting recordings. Specifically, we propose a multi-modal hierarchical attention across three levels: segment, utterance and word. To narrow down the focus into topically-relevant segments, we jointly model topic segmentation and summarization. In addition to traditional text features, we introduce new multi-modal features derived from visual focus of attention, based on the assumption that the utterance is more important if the speaker receives more attention. Experiments show that our model significantly outperforms the state-of-the-art with both BLEU and ROUGE measures.},
	urldate = {2023-05-01},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Li, Manling and Zhang, Lingyu and Ji, Heng and Radke, Richard J.},
	month = jul,
	year = {2019},
	pages = {2190--2196},
}

@misc{bubeck_sparks_2023,
	title = {Sparks of {Artificial} {General} {Intelligence}: {Early} experiments with {GPT}-4},
	shorttitle = {Sparks of {Artificial} {General} {Intelligence}},
	url = {http://arxiv.org/abs/2303.12712},
	abstract = {Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.},
	urldate = {2023-03-24},
	publisher = {arXiv},
	author = {Bubeck, Sébastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco Tulio and Zhang, Yi},
	month = mar,
	year = {2023},
	note = {arXiv:2303.12712 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{gehrmann_repairing_2022,
	title = {Repairing the {Cracked} {Foundation}: {A} {Survey} of {Obstacles} in {Evaluation} {Practices} for {Generated} {Text}},
	shorttitle = {Repairing the {Cracked} {Foundation}},
	url = {http://arxiv.org/abs/2202.06935},
	abstract = {Evaluation practices in natural language generation (NLG) have many known flaws, but improved evaluation approaches are rarely widely adopted. This issue has become more urgent, since neural NLG models have improved to the point where they can often no longer be distinguished based on the surface-level features that older metrics rely on. This paper surveys the issues with human and automatic model evaluations and with commonly used datasets in NLG that have been pointed out over the past 20 years. We summarize, categorize, and discuss how researchers have been addressing these issues and what their findings mean for the current state of model evaluations. Building on those insights, we lay out a long-term vision for NLG evaluation and propose concrete steps for researchers to improve their evaluation processes. Finally, we analyze 66 NLG papers from recent NLP conferences in how well they already follow these suggestions and identify which areas require more drastic changes to the status quo.},
	urldate = {2023-03-24},
	publisher = {arXiv},
	author = {Gehrmann, Sebastian and Clark, Elizabeth and Sellam, Thibault},
	month = feb,
	year = {2022},
	note = {arXiv:2202.06935 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{kikuchi_controlling_2016-1,
	title = {Controlling {Output} {Length} in {Neural} {Encoder}-{Decoders}},
	url = {http://arxiv.org/abs/1609.09552},
	doi = {10.48550/arXiv.1609.09552},
	abstract = {Neural encoder-decoder models have shown great success in many sequence generation tasks. However, previous work has not investigated situations in which we would like to control the length of encoder-decoder outputs. This capability is crucial for applications such as text summarization, in which we have to generate concise summaries with a desired length. In this paper, we propose methods for controlling the output sequence length for neural encoder-decoder models: two decoding-based methods and two learning-based methods. Results show that our learning-based methods have the capability to control length without degrading summary quality in a summarization task.},
	urldate = {2023-03-21},
	publisher = {arXiv},
	author = {Kikuchi, Yuta and Neubig, Graham and Sasano, Ryohei and Takamura, Hiroya and Okumura, Manabu},
	month = sep,
	year = {2016},
	note = {arXiv:1609.09552 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{saito_length-controllable_2020,
	title = {Length-controllable {Abstractive} {Summarization} by {Guiding} with {Summary} {Prototype}},
	url = {http://arxiv.org/abs/2001.07331},
	doi = {10.48550/arXiv.2001.07331},
	abstract = {We propose a new length-controllable abstractive summarization model. Recent state-of-the-art abstractive summarization models based on encoder-decoder models generate only one summary per source text. However, controllable summarization, especially of the length, is an important aspect for practical applications. Previous studies on length-controllable abstractive summarization incorporate length embeddings in the decoder module for controlling the summary length. Although the length embeddings can control where to stop decoding, they do not decide which information should be included in the summary within the length constraint. Unlike the previous models, our length-controllable abstractive summarization model incorporates a word-level extractive module in the encoder-decoder model instead of length embeddings. Our model generates a summary in two steps. First, our word-level extractor extracts a sequence of important words (we call it the "prototype text") from the source text according to the word-level importance scores and the length constraint. Second, the prototype text is used as additional input to the encoder-decoder model, which generates a summary by jointly encoding and copying words from both the prototype text and source text. Since the prototype text is a guide to both the content and length of the summary, our model can generate an informative and length-controlled summary. Experiments with the CNN/Daily Mail dataset and the NEWSROOM dataset show that our model outperformed previous models in length-controlled settings.},
	urldate = {2023-03-21},
	publisher = {arXiv},
	author = {Saito, Itsumi and Nishida, Kyosuke and Nishida, Kosuke and Otsuka, Atsushi and Asano, Hisako and Tomita, Junji and Shindo, Hiroyuki and Matsumoto, Yuji},
	month = jan,
	year = {2020},
	note = {arXiv:2001.07331 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{liu_controlling_2018,
	address = {Brussels, Belgium},
	title = {Controlling {Length} in {Abstractive} {Summarization} {Using} a {Convolutional} {Neural} {Network}},
	url = {https://aclanthology.org/D18-1444},
	doi = {10.18653/v1/D18-1444},
	abstract = {Convolutional neural networks (CNNs) have met great success in abstractive summarization, but they cannot effectively generate summaries of desired lengths. Because generated summaries are used in difference scenarios which may have space or length constraints, the ability to control the summary length in abstractive summarization is an important problem. In this paper, we propose an approach to constrain the summary length by extending a convolutional sequence to sequence model. The results show that this approach generates high-quality summaries with user defined length, and outperforms the baselines consistently in terms of ROUGE score, length variations and semantic similarity.},
	urldate = {2023-03-21},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Liu, Yizhu and Luo, Zhiyi and Zhu, Kenny},
	month = oct,
	year = {2018},
	pages = {4110--4119},
}

@misc{toews_wave_2022,
	title = {A {Wave} {Of} {Billion}-{Dollar} {Language} {AI} {Startups} {Is} {Coming}},
	url = {https://www.forbes.com/sites/robtoews/2022/03/27/a-wave-of-billion-dollar-language-ai-startups-is-coming/},
	abstract = {Given language’s ubiquity throughout the economy, few areas of technology will have a more far-reaching impact in the years ahead than NLP.},
	language = {en},
	urldate = {2023-03-08},
	journal = {Forbes},
	author = {Toews, Rob},
	month = may,
	year = {2022},
	note = {Section: AI},
}

@misc{qin_is_2023,
	title = {Is {ChatGPT} a {General}-{Purpose} {Natural} {Language} {Processing} {Task} {Solver}?},
	url = {http://arxiv.org/abs/2302.06476},
	abstract = {Spurred by advancements in scale, large language models (LLMs) have demonstrated the ability to perform a variety of natural language processing (NLP) tasks zero-shot -- i.e., without adaptation on downstream data. Recently, the debut of ChatGPT has drawn a great deal of attention from the natural language processing (NLP) community due to the fact that it can generate high-quality responses to human input and self-correct previous mistakes based on subsequent conversations. However, it is not yet known whether ChatGPT can serve as a generalist model that can perform many NLP tasks zero-shot. In this work, we empirically analyze the zero-shot learning ability of ChatGPT by evaluating it on 20 popular NLP datasets covering 7 representative task categories. With extensive empirical studies, we demonstrate both the effectiveness and limitations of the current version of ChatGPT. We find that ChatGPT performs well on many tasks favoring reasoning capabilities (e.g., arithmetic reasoning) while it still faces challenges when solving specific tasks such as sequence tagging. We additionally provide in-depth analysis through qualitative case studies.},
	urldate = {2023-03-20},
	publisher = {arXiv},
	author = {Qin, Chengwei and Zhang, Aston and Zhang, Zhuosheng and Chen, Jiaao and Yasunaga, Michihiro and Yang, Diyi},
	month = feb,
	year = {2023},
	note = {arXiv:2302.06476 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{zhang_summn_2022-1,
	title = {Summ{\textasciicircum}{N}: {A} {Multi}-{Stage} {Summarization} {Framework} for {Long} {Input} {Dialogues} and {Documents}},
	shorttitle = {Summ{\textasciicircum}{N}},
	url = {http://arxiv.org/abs/2110.10150},
	abstract = {Text summarization helps readers capture salient information from documents, news, interviews, and meetings. However, most state-of-the-art pretrained language models (LM) are unable to efficiently process long text for many summarization tasks. In this paper, we propose Summ\${\textasciicircum}N\$, a simple, flexible, and effective multi-stage framework for input texts that are longer than the maximum context length of typical pretrained LMs. Summ\${\textasciicircum}N\$ first splits the data samples and generates a coarse summary in multiple stages and then produces the final fine-grained summary based on it. Our framework can process input text of arbitrary length by adjusting the number of stages while keeping the LM input size fixed. Moreover, it can deal with both single-source documents and dialogues, and it can be used on top of different backbone abstractive summarization models. To the best of our knowledge, Summ\${\textasciicircum}N\$ is the first multi-stage split-then-summarize framework for long input summarization. Our experiments demonstrate that Summ\${\textasciicircum}N\$ outperforms previous state-of-the-art methods by improving ROUGE scores on three long meeting summarization datasets AMI, ICSI, and QMSum, two long TV series datasets from SummScreen, and a long document summarization dataset GovReport. Our data and code are available at https://github.com/psunlpgroup/Summ-N.},
	urldate = {2023-03-20},
	publisher = {arXiv},
	author = {Zhang, Yusen and Ni, Ansong and Mao, Ziming and Wu, Chen Henry and Zhu, Chenguang and Deb, Budhaditya and Awadallah, Ahmed H. and Radev, Dragomir and Zhang, Rui},
	month = apr,
	year = {2022},
	note = {arXiv:2110.10150 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{zhao_seal_2020,
	title = {{SEAL}: {Segment}-wise {Extractive}-{Abstractive} {Long}-form {Text} {Summarization}},
	shorttitle = {{SEAL}},
	url = {http://arxiv.org/abs/2006.10213},
	abstract = {Most prior work in the sequence-to-sequence paradigm focused on datasets with input sequence lengths in the hundreds of tokens due to the computational constraints of common RNN and Transformer architectures. In this paper, we study long-form abstractive text summarization, a sequence-to-sequence setting with input sequence lengths up to 100,000 tokens and output sequence lengths up to 768 tokens. We propose SEAL, a Transformer-based model, featuring a new encoder-decoder attention that dynamically extracts/selects input snippets to sparsely attend to for each output segment. Using only the original documents and summaries, we derive proxy labels that provide weak supervision for extractive layers simultaneously with regular supervision from abstractive summaries. The SEAL model achieves state-of-the-art results on existing long-form summarization tasks, and outperforms strong baseline models on a new dataset/task we introduce, Search2Wiki, with much longer input text. Since content selection is explicit in the SEAL model, a desirable side effect is that the selection can be inspected for enhanced interpretability.},
	urldate = {2023-03-20},
	publisher = {arXiv},
	author = {Zhao, Yao and Saleh, Mohammad and Liu, Peter J.},
	month = jun,
	year = {2020},
	note = {arXiv:2006.10213 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Machine Learning},
}

@misc{zhong_searching_2019-1,
	title = {Searching for {Effective} {Neural} {Extractive} {Summarization}: {What} {Works} and {What}'s {Next}},
	shorttitle = {Searching for {Effective} {Neural} {Extractive} {Summarization}},
	url = {http://arxiv.org/abs/1907.03491},
	abstract = {The recent years have seen remarkable success in the use of deep neural networks on text summarization. However, there is no clear understanding of {\textbackslash}textit\{why\} they perform so well, or {\textbackslash}textit\{how\} they might be improved. In this paper, we seek to better understand how neural extractive summarization systems could benefit from different types of model architectures, transferable knowledge and learning schemas. Additionally, we find an effective way to improve current frameworks and achieve the state-of-the-art result on CNN/DailyMail by a large margin based on our observations and analyses. Hopefully, our work could provide more clues for future research on extractive summarization.},
	urldate = {2023-03-20},
	publisher = {arXiv},
	author = {Zhong, Ming and Liu, Pengfei and Wang, Danqing and Qiu, Xipeng and Huang, Xuanjing},
	month = jul,
	year = {2019},
	note = {arXiv:1907.03491 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{see_get_2017-1,
	title = {Get {To} {The} {Point}: {Summarization} with {Pointer}-{Generator} {Networks}},
	shorttitle = {Get {To} {The} {Point}},
	url = {http://arxiv.org/abs/1704.04368},
	doi = {10.48550/arXiv.1704.04368},
	abstract = {Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.},
	urldate = {2023-03-20},
	publisher = {arXiv},
	author = {See, Abigail and Liu, Peter J. and Manning, Christopher D.},
	month = apr,
	year = {2017},
	note = {arXiv:1704.04368 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{sharma_bigpatent_2019,
	address = {Florence, Italy},
	title = {{BIGPATENT}: {A} {Large}-{Scale} {Dataset} for {Abstractive} and {Coherent} {Summarization}},
	shorttitle = {{BIGPATENT}},
	url = {https://aclanthology.org/P19-1212},
	doi = {10.18653/v1/P19-1212},
	abstract = {Most existing text summarization datasets are compiled from the news domain, where summaries have a flattened discourse structure. In such datasets, summary-worthy content often appears in the beginning of input articles. Moreover, large segments from input articles are present verbatim in their respective summaries. These issues impede the learning and evaluation of systems that can understand an article's global content structure as well as produce abstractive summaries with high compression ratio. In this work, we present a novel dataset, BIGPATENT, consisting of 1.3 million records of U.S. patent documents along with human written abstractive summaries. Compared to existing summarization datasets, BIGPATENT has the following properties: i) summaries contain a richer discourse structure with more recurring entities, ii) salient content is evenly distributed in the input, and iii) lesser and shorter extractive fragments are present in the summaries. Finally, we train and evaluate baselines and popular learning models on BIGPATENT to shed light on new challenges and motivate future directions for summarization research.},
	urldate = {2023-03-19},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Sharma, Eva and Li, Chen and Wang, Lu},
	month = jul,
	year = {2019},
	pages = {2204--2213},
}

@misc{cachola_tldr_2020-1,
	title = {{TLDR}: {Extreme} {Summarization} of {Scientific} {Documents}},
	shorttitle = {{TLDR}},
	url = {http://arxiv.org/abs/2004.15011},
	doi = {10.48550/arXiv.2004.15011},
	abstract = {We introduce TLDR generation, a new form of extreme summarization, for scientific papers. TLDR generation involves high source compression and requires expert background knowledge and understanding of complex domain-specific language. To facilitate study on this task, we introduce SciTLDR, a new multi-target dataset of 5.4K TLDRs over 3.2K papers. SciTLDR contains both author-written and expert-derived TLDRs, where the latter are collected using a novel annotation protocol that produces high-quality summaries while minimizing annotation burden. We propose CATTS, a simple yet effective learning strategy for generating TLDRs that exploits titles as an auxiliary training signal. CATTS improves upon strong baselines under both automated metrics and human evaluations. Data and code are publicly available at https://github.com/allenai/scitldr.},
	urldate = {2023-03-19},
	publisher = {arXiv},
	author = {Cachola, Isabel and Lo, Kyle and Cohan, Arman and Weld, Daniel S.},
	month = oct,
	year = {2020},
	note = {arXiv:2004.15011 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{li_competition-level_2022,
	title = {Competition-{Level} {Code} {Generation} with {AlphaCode}},
	volume = {378},
	issn = {0036-8075, 1095-9203},
	url = {http://arxiv.org/abs/2203.07814},
	doi = {10.1126/science.abq1158},
	abstract = {Programming is a powerful and ubiquitous problem-solving tool. Developing systems that can assist programmers or even generate programs independently could make programming more productive and accessible, yet so far incorporating innovations in AI has proven challenging. Recent large-scale language models have demonstrated an impressive ability to generate code, and are now able to complete simple programming tasks. However, these models still perform poorly when evaluated on more complex, unseen problems that require problem-solving skills beyond simply translating instructions into code. For example, competitive programming problems which require an understanding of algorithms and complex natural language remain extremely challenging. To address this gap, we introduce AlphaCode, a system for code generation that can create novel solutions to these problems that require deeper reasoning. In simulated evaluations on recent programming competitions on the Codeforces platform, AlphaCode achieved on average a ranking of top 54.3\% in competitions with more than 5,000 participants. We found that three key components were critical to achieve good and reliable performance: (1) an extensive and clean competitive programming dataset for training and evaluation, (2) large and efficient-to-sample transformer-based architectures, and (3) large-scale model sampling to explore the search space, followed by filtering based on program behavior to a small set of submissions.},
	number = {6624},
	urldate = {2023-03-19},
	journal = {Science},
	author = {Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, Rémi and Eccles, Tom and Keeling, James and Gimeno, Felix and Lago, Agustin Dal and Hubert, Thomas and Choy, Peter and d'Autume, Cyprien de Masson and Babuschkin, Igor and Chen, Xinyun and Huang, Po-Sen and Welbl, Johannes and Gowal, Sven and Cherepanov, Alexey and Molloy, James and Mankowitz, Daniel J. and Robson, Esme Sutherland and Kohli, Pushmeet and de Freitas, Nando and Kavukcuoglu, Koray and Vinyals, Oriol},
	month = dec,
	year = {2022},
	note = {arXiv:2203.07814 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Programming Languages},
	pages = {1092--1097},
}

@misc{replit_ghostwriter_nodate,
	title = {Ghostwriter {AI} \& {Complete} {Code} {Beta}},
	url = {https://blog.replit.com/ai},
	abstract = {Update: Ghostwriter is out now!…},
	urldate = {2023-03-19},
	journal = {Replit Blog},
	author = {repl.it},
}

@misc{parthasarathy_zooms_2023,
	title = {Zoom’s {AI} innovations empower people},
	url = {https://blog.zoom.us/ai-driven-innovations/},
	abstract = {Zoom's smart recording feature uses AI to help make meetings more actionable and efficient by creating skimmable smart chapters.},
	language = {en-US},
	urldate = {2023-03-16},
	journal = {Zoom Blog},
	author = {Parthasarathy, Vijay},
	month = feb,
	year = {2023},
}

@misc{midha_discord_2023,
	title = {Discord is {Your} {Place} for {AI} with {Friends}},
	url = {https://discord.com/blog/ai-on-discord-your-place-for-ai-with-friends},
	abstract = {Today, we’re announcing new AI experiments, including an AI chatbot called Clyde, AutoMod AI, and Conversation Summaries, and launching an AI Incubator.},
	language = {en},
	urldate = {2023-03-09},
	author = {Midha, Anjney},
	month = mar,
	year = {2023},
}

@misc{colombo_glass_2022,
	title = {The {Glass} {Ceiling} of {Automatic} {Evaluation} in {Natural} {Language} {Generation}},
	url = {http://arxiv.org/abs/2208.14585},
	abstract = {Automatic evaluation metrics capable of replacing human judgments are critical to allowing fast development of new methods. Thus, numerous research efforts have focused on crafting such metrics. In this work, we take a step back and analyze recent progress by comparing the body of existing automatic metrics and human metrics altogether. As metrics are used based on how they rank systems, we compare metrics in the space of system rankings. Our extensive statistical analysis reveals surprising findings: automatic metrics -- old and new -- are much more similar to each other than to humans. Automatic metrics are not complementary and rank systems similarly. Strikingly, human metrics predict each other much better than the combination of all automatic metrics used to predict a human metric. It is surprising because human metrics are often designed to be independent, to capture different aspects of quality, e.g. content fidelity or readability. We provide a discussion of these findings and recommendations for future work in the field of evaluation.},
	urldate = {2023-03-16},
	publisher = {arXiv},
	author = {Colombo, Pierre and Peyrard, Maxime and Noiry, Nathan and West, Robert and Piantanida, Pablo},
	month = oct,
	year = {2022},
	note = {arXiv:2208.14585 [cs]
	version: 2},
	keywords = {Computer Science - Computation and Language},
}

@article{gidiotis_divide-and-conquer_2020,
	title = {A {Divide}-and-{Conquer} {Approach} to the {Summarization} of {Long} {Documents}},
	volume = {28},
	issn = {2329-9304},
	doi = {10.1109/TASLP.2020.3037401},
	abstract = {We present a novel divide-and-conquer method for the neural summarization of long documents. Our method exploits the discourse structure of the document and uses sentence similarity to split the problem into an ensemble of smaller summarization problems. In particular, we break a long document and its summary into multiple source-target pairs, which are used for training a model that learns to summarize each part of the document separately. These partial summaries are then combined in order to produce a final complete summary. With this approach we can decompose the problem of long document summarization into smaller and simpler problems, reducing computational complexity and creating more training examples, which at the same time contain less noise in the target summaries compared to the standard approach. We demonstrate that this approach paired with different summarization models, including sequence-to-sequence RNNs and Transformers, can lead to improved summarization performance. Our best models achieve results that are on par with the state-of-the-art in two two publicly available datasets of academic articles.},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Gidiotis, Alexios and Tsoumakas, Grigorios},
	year = {2020},
	note = {Conference Name: IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	keywords = {Computational modeling, Convergence, Decoding, Speech processing, Stability analysis, Standards, Summarization of long documents, Training, deep learning, natural language processing, neural summarization, text summarization},
	pages = {3029--3040},
}

@misc{warren_discord_2023,
	title = {Discord starts testing {ChatGPT}-powered {Clyde} chatbot and other {AI} features},
	url = {https://www.theverge.com/2023/3/9/23631930/discord-openai-clyde-chatbot-automod-features-ai},
	abstract = {Discord wants its platform to be the home of AI developers.},
	language = {en-US},
	urldate = {2023-03-09},
	journal = {The Verge},
	author = {Warren, Tom},
	month = mar,
	year = {2023},
}

@misc{noauthor_otters_nodate,
	title = {Otter’s {New} {OtterPilot}™ is the {Smart} {AI} {Meeting} {Assistant} {You}’ve {Been} {Waiting} {For} {\textbar} {Otter}.ai},
	url = {https://otter.ai/blog/otters-new-otterpilot-tm-is-the-smart-ai-meeting-assistant-youve-been-waiting-for},
	abstract = {Otter’s new OtterPilot™ is the smart AI meeting assistant that automates meetings from start to finish.},
	language = {en},
	urldate = {2023-03-09},
}

@misc{butcher_heres_2022,
	title = {Here's why a gold rush of {NLP} startups is about to arrive},
	url = {https://techcrunch.com/2022/07/28/a-gold-rush-of-nlp-startups-is-about-to-arrive-heres-why/},
	abstract = {Remember Natural Language Processing? NLP arose several years ago but it was only in 2018 that AI researchers proved it was possible to train a neural network once on a large amount of data and use it again and again for different tasks. In 2019 GPT-2 from Open AI, and T5 by Google appeared, showing […]},
	language = {en-US},
	urldate = {2023-03-08},
	journal = {TechCrunch},
	author = {Butcher, Mike},
	month = jul,
	year = {2022},
}

@misc{kumar_meeting_2022,
	title = {Meeting {Summarization}: {A} {Survey} of the {State} of the {Art}},
	shorttitle = {Meeting {Summarization}},
	url = {http://arxiv.org/abs/2212.08206},
	abstract = {Information overloading requires the need for summarizers to extract salient information from the text. Currently, there is an overload of dialogue data due to the rise of virtual communication platforms. The rise of Covid-19 has led people to rely on online communication platforms like Zoom, Slack, Microsoft Teams, Discord, etc. to conduct their company meetings. Instead of going through the entire meeting transcripts, people can use meeting summarizers to select useful data. Nevertheless, there is a lack of comprehensive surveys in the field of meeting summarizers. In this survey, we aim to cover recent meeting summarization techniques. Our survey offers a general overview of text summarization along with datasets and evaluation metrics for meeting summarization. We also provide the performance of each summarizer on a leaderboard. We conclude our survey with different challenges in this domain and potential research opportunities for future researchers.},
	urldate = {2023-03-07},
	publisher = {arXiv},
	author = {Kumar, Lakshmi Prasanna and Kabiri, Arman},
	month = dec,
	year = {2022},
	note = {arXiv:2212.08206 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{pagnoni_socratic_2022,
	title = {Socratic {Pretraining}: {Question}-{Driven} {Pretraining} for {Controllable} {Summarization}},
	shorttitle = {Socratic {Pretraining}},
	url = {http://arxiv.org/abs/2212.10449},
	doi = {10.48550/arXiv.2212.10449},
	abstract = {In long document controllable summarization, where labeled data is scarce, pretrained models struggle to adapt to the task and effectively respond to user queries. In this paper, we introduce Socratic pretraining, a question-driven, unsupervised pretraining objective specifically designed to improve controllability in summarization tasks. By training a model to generate and answer relevant questions in a given context, Socratic pretraining enables the model to more effectively adhere to user-provided queries and identify relevant content to be summarized. We demonstrate the effectiveness of this approach through extensive experimentation on two summarization domains, short stories and dialogue, and multiple control strategies: keywords, questions, and factoid QA pairs. Our pretraining method relies only on unlabeled documents and a question generation system and outperforms pre-finetuning approaches that use additional supervised data. Furthermore, our results show that Socratic pretraining cuts task-specific labeled data requirements in half, is more faithful to user-provided queries, and achieves state-of-the-art performance on QMSum and SQuALITY.},
	urldate = {2023-03-07},
	publisher = {arXiv},
	author = {Pagnoni, Artidoro and Fabbri, Alexander R. and Kryściński, Wojciech and Wu, Chien-Sheng},
	month = dec,
	year = {2022},
	note = {arXiv:2212.10449 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{xiao_attend_2022,
	title = {Attend to the {Right} {Context}: {A} {Plug}-and-{Play} {Module} for {Content}-{Controllable} {Summarization}},
	shorttitle = {Attend to the {Right} {Context}},
	url = {http://arxiv.org/abs/2212.10819},
	doi = {10.48550/arXiv.2212.10819},
	abstract = {Content-Controllable Summarization generates summaries focused on the given controlling signals. Due to the lack of large-scale training corpora for the task, we propose a plug-and-play module RelAttn to adapt any general summarizers to the content-controllable summarization task. RelAttn first identifies the relevant content in the source documents, and then makes the model attend to the right context by directly steering the attention weight. We further apply an unsupervised online adaptive parameter searching algorithm to determine the degree of control in the zero-shot setting, while such parameters are learned in the few-shot setting. By applying the module to three backbone summarization models, experiments show that our method effectively improves all the summarizers, and outperforms the prefix-based method and a widely used plug-and-play model in both zero- and few-shot settings. Tellingly, more benefit is observed in the scenarios when more control is needed.},
	urldate = {2023-03-07},
	publisher = {arXiv},
	author = {Xiao, Wen and Miculicich, Lesly and Liu, Yang and He, Pengcheng and Carenini, Giuseppe},
	month = dec,
	year = {2022},
	note = {arXiv:2212.10819 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{verma_large_2023,
	title = {Large {Scale} {Multi}-{Lingual} {Multi}-{Modal} {Summarization} {Dataset}},
	url = {http://arxiv.org/abs/2302.06560},
	doi = {10.48550/arXiv.2302.06560},
	abstract = {Significant developments in techniques such as encoder-decoder models have enabled us to represent information comprising multiple modalities. This information can further enhance many downstream tasks in the field of information retrieval and natural language processing; however, improvements in multi-modal techniques and their performance evaluation require large-scale multi-modal data which offers sufficient diversity. Multi-lingual modeling for a variety of tasks like multi-modal summarization, text generation, and translation leverages information derived from high-quality multi-lingual annotated data. In this work, we present the current largest multi-lingual multi-modal summarization dataset (M3LS), and it consists of over a million instances of document-image pairs along with a professionally annotated multi-modal summary for each pair. It is derived from news articles published by British Broadcasting Corporation(BBC) over a decade and spans 20 languages, targeting diversity across five language roots, it is also the largest summarization dataset for 13 languages and consists of cross-lingual summarization data for 2 languages. We formally define the multi-lingual multi-modal summarization task utilizing our dataset and report baseline scores from various state-of-the-art summarization techniques in a multi-lingual setting. We also compare it with many similar datasets to analyze the uniqueness and difficulty of M3LS.},
	urldate = {2023-03-07},
	publisher = {arXiv},
	author = {Verma, Yash and Jangra, Anubhav and Kumar, Raghvendra and Saha, Sriparna},
	month = feb,
	year = {2023},
	note = {arXiv:2302.06560 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Multimedia},
}

@misc{matsuura_leveraging_2023,
	title = {Leveraging {Large} {Text} {Corpora} for {End}-to-{End} {Speech} {Summarization}},
	url = {http://arxiv.org/abs/2303.00978},
	doi = {10.48550/arXiv.2303.00978},
	abstract = {End-to-end speech summarization (E2E SSum) is a technique to directly generate summary sentences from speech. Compared with the cascade approach, which combines automatic speech recognition (ASR) and text summarization models, the E2E approach is more promising because it mitigates ASR errors, incorporates nonverbal information, and simplifies the overall system. However, since collecting a large amount of paired data (i.e., speech and summary) is difficult, the training data is usually insufficient to train a robust E2E SSum system. In this paper, we present two novel methods that leverage a large amount of external text summarization data for E2E SSum training. The first technique is to utilize a text-to-speech (TTS) system to generate synthesized speech, which is used for E2E SSum training with the text summary. The second is a TTS-free method that directly inputs phoneme sequence instead of synthesized speech to the E2E SSum model. Experiments show that our proposed TTS- and phoneme-based methods improve several metrics on the How2 dataset. In particular, our best system outperforms a previous state-of-the-art one by a large margin (i.e., METEOR score improvements of more than 6 points). To the best of our knowledge, this is the first work to use external language resources for E2E SSum. Moreover, we report a detailed analysis of the How2 dataset to confirm the validity of our proposed E2E SSum system.},
	urldate = {2023-03-07},
	publisher = {arXiv},
	author = {Matsuura, Kohei and Ashihara, Takanori and Moriya, Takafumi and Tanaka, Tomohiro and Ogawa, Atsunori and Delcroix, Marc and Masumura, Ryo},
	month = mar,
	year = {2023},
	note = {arXiv:2303.00978 [cs, eess]},
	keywords = {Computer Science - Computation and Language, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{wan_faithfulness-aware_2023,
	title = {Faithfulness-{Aware} {Decoding} {Strategies} for {Abstractive} {Summarization}},
	url = {http://arxiv.org/abs/2303.03278},
	abstract = {Despite significant progress in understanding and improving faithfulness in abstractive summarization, the question of how decoding strategies affect faithfulness is less studied. We present a systematic study of the effect of generation techniques such as beam search and nucleus sampling on faithfulness in abstractive summarization. We find a consistent trend where beam search with large beam sizes produces the most faithful summaries while nucleus sampling generates the least faithful ones. We propose two faithfulness-aware generation methods to further improve faithfulness over current generation techniques: (1) ranking candidates generated by beam search using automatic faithfulness metrics and (2) incorporating lookahead heuristics that produce a faithfulness score on the future summary. We show that both generation methods significantly improve faithfulness across two datasets as evaluated by four automatic faithfulness metrics and human evaluation. To reduce computational cost, we demonstrate a simple distillation approach that allows the model to generate faithful summaries with just greedy decoding. Our code is publicly available at https://github.com/amazon-science/faithful-summarization-generation},
	urldate = {2023-03-07},
	publisher = {arXiv},
	author = {Wan, David and Liu, Mengwen and McKeown, Kathleen and Dreyer, Markus and Bansal, Mohit},
	month = mar,
	year = {2023},
	note = {arXiv:2303.03278 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{liu_length_2022,
	address = {Dublin, Ireland},
	title = {Length {Control} in {Abstractive} {Summarization} by {Pretraining} {Information} {Selection}},
	url = {https://aclanthology.org/2022.acl-long.474},
	doi = {10.18653/v1/2022.acl-long.474},
	abstract = {Previous length-controllable summarization models mostly control lengths at the decoding stage, whereas the encoding or the selection of information from the source document is not sensitive to the designed length. They also tend to generate summaries as long as those in the training data. In this paper, we propose a length-aware attention mechanism (LAAM) to adapt the encoding of the source based on the desired length. Our approach works by training LAAM on a summary length balanced dataset built from the original training data, and then fine-tuning as usual. Results show that this approach is effective in generating high-quality summaries with desired lengths and even those short lengths never seen in the original training set.},
	urldate = {2023-03-07},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Liu, Yizhu and Jia, Qi and Zhu, Kenny},
	month = may,
	year = {2022},
	pages = {6885--6895},
}

@misc{he_ctrlsum_2020,
	title = {{CTRLsum}: {Towards} {Generic} {Controllable} {Text} {Summarization}},
	shorttitle = {{CTRLsum}},
	url = {http://arxiv.org/abs/2012.04281},
	doi = {10.48550/arXiv.2012.04281},
	abstract = {Current summarization systems yield generic summaries that are disconnected from users' preferences and expectations. To address this limitation, we present CTRLsum, a novel framework for controllable summarization. Our approach enables users to control multiple aspects of generated summaries by interacting with the summarization system through textual input in the form of a set of keywords or descriptive prompts. Using a single unified model, CTRLsum is able to achieve a broad scope of summary manipulation at inference time without requiring additional human annotations or pre-defining a set of control aspects during training. We quantitatively demonstrate the effectiveness of our approach on three domains of summarization datasets and five control aspects: 1) entity-centric and 2) length-controllable summarization, 3) contribution summarization on scientific papers, 4) invention purpose summarization on patent filings, and 5) question-guided summarization on news articles in a reading comprehension setting. Moreover, when used in a standard, uncontrolled summarization setting, CTRLsum achieves state-of-the-art results on the CNN/DailyMail dataset. Code and model checkpoints are available at https://github.com/salesforce/ctrl-sum},
	urldate = {2023-03-07},
	publisher = {arXiv},
	author = {He, Junxian and Kryściński, Wojciech and McCann, Bryan and Rajani, Nazneen and Xiong, Caiming},
	month = dec,
	year = {2020},
	note = {arXiv:2012.04281 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{ladhak_exploring_2021,
	title = {Exploring {Content} {Selection} in {Summarization} of {Novel} {Chapters}},
	url = {http://arxiv.org/abs/2005.01840},
	abstract = {We present a new summarization task, generating summaries of novel chapters using summary/chapter pairs from online study guides. This is a harder task than the news summarization task, given the chapter length as well as the extreme paraphrasing and generalization found in the summaries. We focus on extractive summarization, which requires the creation of a gold-standard set of extractive summaries. We present a new metric for aligning reference summary sentences with chapter sentences to create gold extracts and also experiment with different alignment methods. Our experiments demonstrate significant improvement over prior alignment approaches for our task as shown through automatic metrics and a crowd-sourced pyramid analysis. We make our data collection scripts available at https://github.com/manestay/novel-chapter-dataset .},
	urldate = {2023-03-07},
	publisher = {arXiv},
	author = {Ladhak, Faisal and Li, Bryan and Al-Onaizan, Yaser and McKeown, Kathleen},
	month = mar,
	year = {2021},
	note = {arXiv:2005.01840 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{wu_recursively_2021,
	title = {Recursively {Summarizing} {Books} with {Human} {Feedback}},
	url = {http://arxiv.org/abs/2109.10862},
	doi = {10.48550/arXiv.2109.10862},
	abstract = {A major challenge for scaling machine learning is training models to perform tasks that are very difficult or time-consuming for humans to evaluate. We present progress on this problem on the task of abstractive summarization of entire fiction novels. Our method combines learning from human feedback with recursive task decomposition: we use models trained on smaller parts of the task to assist humans in giving feedback on the broader task. We collect a large volume of demonstrations and comparisons from human labelers, and fine-tune GPT-3 using behavioral cloning and reward modeling to do summarization recursively. At inference time, the model first summarizes small sections of the book and then recursively summarizes these summaries to produce a summary of the entire book. Our human labelers are able to supervise and evaluate the models quickly, despite not having read the entire books themselves. Our resulting model generates sensible summaries of entire books, even matching the quality of human-written summaries in a few cases (\${\textbackslash}sim5{\textbackslash}\%\$ of books). We achieve state-of-the-art results on the recent BookSum dataset for book-length summarization. A zero-shot question-answering model using these summaries achieves state-of-the-art results on the challenging NarrativeQA benchmark for answering questions about books and movie scripts. We release datasets of samples from our model.},
	urldate = {2023-03-07},
	publisher = {arXiv},
	author = {Wu, Jeff and Ouyang, Long and Ziegler, Daniel M. and Stiennon, Nisan and Lowe, Ryan and Leike, Jan and Christiano, Paul},
	month = sep,
	year = {2021},
	note = {arXiv:2109.10862 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{ji_survey_2023,
	title = {Survey of {Hallucination} in {Natural} {Language} {Generation}},
	volume = {55},
	issn = {0360-0300, 1557-7341},
	url = {http://arxiv.org/abs/2202.03629},
	doi = {10.1145/3571730},
	abstract = {Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before. In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions; and (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, machine translation, and visual-language generation. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG.},
	number = {12},
	urldate = {2023-03-06},
	journal = {ACM Computing Surveys},
	author = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Yejin and Dai, Wenliang and Madotto, Andrea and Fung, Pascale},
	month = dec,
	year = {2023},
	note = {arXiv:2202.03629 [cs]},
	keywords = {A.1, Computer Science - Computation and Language},
	pages = {1--38},
}

@misc{he_z-code_2022,
	title = {Z-{Code}++: {A} {Pre}-trained {Language} {Model} {Optimized} for {Abstractive} {Summarization}},
	shorttitle = {Z-{Code}++},
	url = {http://arxiv.org/abs/2208.09770},
	abstract = {This paper presents Z-Code++, a new pre-trained language model optimized for abstractive text summarization. The model extends the state of the art encoder-decoder model using three techniques. First, we use a two-phase pre-training process to improve model's performance on low-resource summarization tasks. The model is first pre-trained using text corpora for language understanding, and then is continually pre-trained on summarization corpora for grounded text generation. Second, we replace self-attention layers in the encoder with disentangled attention layers, where each word is represented using two vectors that encode its content and position, respectively. Third, we use fusion-in-encoder, a simple yet effective method of encoding long sequences in a hierarchical manner. Z-Code++ creates new state of the art on 9 out of 13 text summarization tasks across 5 languages. Our model is parameter-efficient in that it outperforms the 600x larger PaLM-540B on XSum, and the finetuned 200x larger GPT3-175B on SAMSum. In zero-shot and few-shot settings, our model substantially outperforms the competing models.},
	urldate = {2023-03-06},
	publisher = {arXiv},
	author = {He, Pengcheng and Peng, Baolin and Lu, Liyang and Wang, Song and Mei, Jie and Liu, Yang and Xu, Ruochen and Awadalla, Hany Hassan and Shi, Yu and Zhu, Chenguang and Xiong, Wayne and Zeng, Michael and Gao, Jianfeng and Huang, Xuedong},
	month = aug,
	year = {2022},
	note = {arXiv:2208.09770 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, I.2, I.7, cs.CL, cs.GL},
}

@misc{mehta_brave_2023,
	title = {Brave {Search} launches an {AI}-powered summarization feature},
	url = {https://techcrunch.com/2023/03/02/brave-search-launches-an-ai-powered-summarization-feature/},
	abstract = {Brave Search launched a new "Summarizer" feature, which is powered by different large langue models (LLMs) than OpenAI's GPT.},
	language = {en-US},
	urldate = {2023-03-03},
	journal = {TechCrunch},
	author = {Mehta, Ivan},
	month = mar,
	year = {2023},
}

@misc{beltagy_longformer_2020,
	title = {Longformer: {The} {Long}-{Document} {Transformer}},
	shorttitle = {Longformer},
	url = {http://arxiv.org/abs/2004.05150},
	abstract = {Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.},
	urldate = {2023-03-03},
	publisher = {arXiv},
	author = {Beltagy, Iz and Peters, Matthew E. and Cohan, Arman},
	month = dec,
	year = {2020},
	note = {arXiv:2004.05150 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{kryscinski_evaluating_2019,
	title = {Evaluating the {Factual} {Consistency} of {Abstractive} {Text} {Summarization}},
	url = {http://arxiv.org/abs/1910.12840},
	abstract = {Currently used metrics for assessing summarization algorithms do not account for whether summaries are factually consistent with source documents. We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between source documents and a generated summary. Training data is generated by applying a series of rule-based transformations to the sentences of source documents. The factual consistency model is then trained jointly for three tasks: 1) identify whether sentences remain factually consistent after transformation, 2) extract a span in the source documents to support the consistency prediction, 3) extract a span in the summary sentence that is inconsistent if one exists. Transferring this model to summaries generated by several state-of-the art models reveals that this highly scalable approach substantially outperforms previous models, including those trained with strong supervision using standard datasets for natural language inference and fact checking. Additionally, human evaluation shows that the auxiliary span extraction tasks provide useful assistance in the process of verifying factual consistency.},
	urldate = {2023-03-03},
	publisher = {arXiv},
	author = {Kryściński, Wojciech and McCann, Bryan and Xiong, Caiming and Socher, Richard},
	month = oct,
	year = {2019},
	note = {arXiv:1910.12840 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{kadavath_language_2022,
	title = {Language {Models} ({Mostly}) {Know} {What} {They} {Know}},
	url = {http://arxiv.org/abs/2207.05221},
	doi = {10.48550/arXiv.2207.05221},
	abstract = {We study whether language models can evaluate the validity of their own claims and predict which questions they will be able to answer correctly. We first show that larger models are well-calibrated on diverse multiple choice and true/false questions when they are provided in the right format. Thus we can approach self-evaluation on open-ended sampling tasks by asking models to first propose answers, and then to evaluate the probability "P(True)" that their answers are correct. We find encouraging performance, calibration, and scaling for P(True) on a diverse array of tasks. Performance at self-evaluation further improves when we allow models to consider many of their own samples before predicting the validity of one specific possibility. Next, we investigate whether models can be trained to predict "P(IK)", the probability that "I know" the answer to a question, without reference to any particular proposed answer. Models perform well at predicting P(IK) and partially generalize across tasks, though they struggle with calibration of P(IK) on new tasks. The predicted P(IK) probabilities also increase appropriately in the presence of relevant source materials in the context, and in the presence of hints towards the solution of mathematical word problems. We hope these observations lay the groundwork for training more honest models, and for investigating how honesty generalizes to cases where models are trained on objectives other than the imitation of human writing.},
	urldate = {2023-03-02},
	publisher = {arXiv},
	author = {Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Hatfield-Dodds, Zac and DasSarma, Nova and Tran-Johnson, Eli and Johnston, Scott and El-Showk, Sheer and Jones, Andy and Elhage, Nelson and Hume, Tristan and Chen, Anna and Bai, Yuntao and Bowman, Sam and Fort, Stanislav and Ganguli, Deep and Hernandez, Danny and Jacobson, Josh and Kernion, Jackson and Kravec, Shauna and Lovitt, Liane and Ndousse, Kamal and Olsson, Catherine and Ringer, Sam and Amodei, Dario and Brown, Tom and Clark, Jack and Joseph, Nicholas and Mann, Ben and McCandlish, Sam and Olah, Chris and Kaplan, Jared},
	month = nov,
	year = {2022},
	note = {arXiv:2207.05221 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{liu_revisiting_2022,
	title = {Revisiting the {Gold} {Standard}: {Grounding} {Summarization} {Evaluation} with {Robust} {Human} {Evaluation}},
	shorttitle = {Revisiting the {Gold} {Standard}},
	url = {http://arxiv.org/abs/2212.07981},
	doi = {10.48550/arXiv.2212.07981},
	abstract = {Human evaluation is the foundation upon which the evaluation of both summarization systems and automatic metrics rests. However, existing human evaluation protocols and benchmarks for summarization either exhibit low inter-annotator agreement or lack the scale needed to draw statistically significant conclusions, and an in-depth analysis of human evaluation is lacking. In this work, we address the shortcomings of existing summarization evaluation along the following axes: 1) We propose a modified summarization salience protocol, Atomic Content Units (ACUs), which relies on fine-grained semantic units and allows for high inter-annotator agreement. 2) We curate the Robust Summarization Evaluation (RoSE) benchmark, a large human evaluation dataset consisting of over 22k summary-level annotations over state-of-the-art systems on three datasets. 3) We compare our ACU protocol with three other human evaluation protocols, underscoring potential confounding factors in evaluation setups. 4) We evaluate existing automatic metrics using the collected human annotations across evaluation protocols and demonstrate how our benchmark leads to more statistically stable and significant results. Furthermore, our findings have important implications for evaluating large language models (LLMs), as we show that LLMs adjusted by human feedback (e.g., GPT-3.5) may overfit unconstrained human evaluation, which is affected by the annotators' prior, input-agnostic preferences, calling for more robust, targeted evaluation methods.},
	urldate = {2023-03-02},
	publisher = {arXiv},
	author = {Liu, Yixin and Fabbri, Alexander R. and Liu, Pengfei and Zhao, Yilun and Nan, Linyong and Han, Ruilin and Han, Simeng and Joty, Shafiq and Wu, Chien-Sheng and Xiong, Caiming and Radev, Dragomir},
	month = dec,
	year = {2022},
	note = {arXiv:2212.07981 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{bhaskar_zero-shot_2022,
	title = {Zero-{Shot} {Opinion} {Summarization} with {GPT}-3},
	url = {http://arxiv.org/abs/2211.15914},
	doi = {10.48550/arXiv.2211.15914},
	abstract = {Very large language models such as GPT-3 have shown impressive performance across a wide variety of tasks, including text summarization. In this paper, we show that this strong performance extends to opinion summarization. We explore several pipeline methods for applying GPT-3 to summarize a large collection of user reviews in a zero-shot fashion, notably approaches based on recursive summarization and selecting salient content to summarize through supervised clustering or extraction. On two datasets, an aspect-oriented summarization dataset of hotel reviews and a generic summarization dataset of Amazon and Yelp reviews, we show that the GPT-3 models achieve very strong performance in human evaluation. We argue that standard evaluation metrics do not reflect this, and evaluate against several new measures targeting faithfulness, factuality, and genericity to contrast these different methods.},
	urldate = {2023-03-02},
	publisher = {arXiv},
	author = {Bhaskar, Adithya and Fabbri, Alexander R. and Durrett, Greg},
	month = nov,
	year = {2022},
	note = {arXiv:2211.15914 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{kocmi_large_2023,
	title = {Large {Language} {Models} {Are} {State}-of-the-{Art} {Evaluators} of {Translation} {Quality}},
	url = {http://arxiv.org/abs/2302.14520},
	doi = {10.48550/arXiv.2302.14520},
	abstract = {We describe GEMBA, a GPT-based metric for assessment of translation quality, which works both with a reference translation and without. In our evaluation, we focus on zero-shot prompting, comparing four prompt variants in two modes, based on the availability of the reference. We investigate seven versions of GPT models, including ChatGPT. We show that our method for translation quality assessment only works with GPT 3.5 and larger models. Comparing to results from WMT22's Metrics shared task, our method achieves state-of-the-art accuracy in both modes when compared to MQM-based human labels. Our results are valid on the system level for all three WMT22 Metrics shared task language pairs, namely English into German, English into Russian, and Chinese into English. This provides a first glimpse into the usefulness of pre-trained, generative large language models for quality assessment of translations. We publicly release all our code and prompt templates used for the experiments described in this work, as well as all corresponding scoring results, to allow for external validation and reproducibility.},
	urldate = {2023-03-02},
	publisher = {arXiv},
	author = {Kocmi, Tom and Federmann, Christian},
	month = feb,
	year = {2023},
	note = {arXiv:2302.14520 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{kocmi_large_nodate,
	title = {Large {Language} {Models} {Are} {State}-of-the-{Art} {Evaluators} of {Translation} {Quality}},
	language = {en},
	author = {Kocmi, Tom and Federmann, Christian},
}

@misc{wang_superglue_2020,
	title = {{SuperGLUE}: {A} {Stickier} {Benchmark} for {General}-{Purpose} {Language} {Understanding} {Systems}},
	shorttitle = {{SuperGLUE}},
	url = {http://arxiv.org/abs/1905.00537},
	abstract = {In the last year, new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understanding tasks. The GLUE benchmark, introduced a little over one year ago, offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently surpassed the level of non-expert humans, suggesting limited headroom for further research. In this paper we present SuperGLUE, a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, a software toolkit, and a public leaderboard. SuperGLUE is available at super.gluebenchmark.com.},
	urldate = {2023-03-02},
	publisher = {arXiv},
	author = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
	month = feb,
	year = {2020},
	note = {arXiv:1905.00537 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{wang_superglue_nodate,
	title = {{SuperGLUE}: {A} {Stickier} {Benchmark} for {General}-{Purpose} {Language} {Understanding} {Systems}},
	abstract = {In the last year, new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understanding tasks. The GLUE benchmark, introduced a little over one year ago, offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently surpassed the level of non-expert humans, suggesting limited headroom for further research. In this paper we present SuperGLUE, a new benchmark styled after GLUE with a new set of more difﬁcult language understanding tasks, a software toolkit, and a public leaderboard. SuperGLUE is available at super.gluebenchmark.com.},
	language = {en},
	author = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
}

@misc{hendrycks_measuring_2021,
	title = {Measuring {Massive} {Multitask} {Language} {Understanding}},
	url = {http://arxiv.org/abs/2009.03300},
	abstract = {We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.},
	urldate = {2023-03-02},
	publisher = {arXiv},
	author = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
	month = jan,
	year = {2021},
	note = {arXiv:2009.03300 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@misc{liu_revisiting_2022-1,
	title = {Revisiting the {Gold} {Standard}: {Grounding} {Summarization} {Evaluation} with {Robust} {Human} {Evaluation}},
	shorttitle = {Revisiting the {Gold} {Standard}},
	url = {http://arxiv.org/abs/2212.07981},
	doi = {10.48550/arXiv.2212.07981},
	abstract = {Human evaluation is the foundation upon which the evaluation of both summarization systems and automatic metrics rests. However, existing human evaluation protocols and benchmarks for summarization either exhibit low inter-annotator agreement or lack the scale needed to draw statistically significant conclusions, and an in-depth analysis of human evaluation is lacking. In this work, we address the shortcomings of existing summarization evaluation along the following axes: 1) We propose a modified summarization salience protocol, Atomic Content Units (ACUs), which relies on fine-grained semantic units and allows for high inter-annotator agreement. 2) We curate the Robust Summarization Evaluation (RoSE) benchmark, a large human evaluation dataset consisting of over 22k summary-level annotations over state-of-the-art systems on three datasets. 3) We compare our ACU protocol with three other human evaluation protocols, underscoring potential confounding factors in evaluation setups. 4) We evaluate existing automatic metrics using the collected human annotations across evaluation protocols and demonstrate how our benchmark leads to more statistically stable and significant results. Furthermore, our findings have important implications for evaluating large language models (LLMs), as we show that LLMs adjusted by human feedback (e.g., GPT-3.5) may overfit unconstrained human evaluation, which is affected by the annotators' prior, input-agnostic preferences, calling for more robust, targeted evaluation methods.},
	urldate = {2023-03-02},
	publisher = {arXiv},
	author = {Liu, Yixin and Fabbri, Alexander R. and Liu, Pengfei and Zhao, Yilun and Nan, Linyong and Han, Ruilin and Han, Simeng and Joty, Shafiq and Wu, Chien-Sheng and Xiong, Caiming and Radev, Dragomir},
	month = dec,
	year = {2022},
	note = {arXiv:2212.07981 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{wiggers_thegist_2022,
	title = {{TheGist} taps {AI} to summarize {Slack} channels and threads},
	url = {https://techcrunch.com/2022/11/15/thegist-uses-ai-to-summarize-slack-channels-and-threads/},
	abstract = {TheGist is a new startup developing an app that can summarize Slack conversations. So far, it's raised several million dollars.},
	language = {en-US},
	urldate = {2023-03-01},
	journal = {TechCrunch},
	author = {Wiggers, Kyle},
	month = nov,
	year = {2022},
}

@misc{wenger_ai_2023,
	title = {{AI} {Email} {Summaries}: {Read} emails in seconds},
	shorttitle = {{AI} {Email} {Summaries}},
	url = {https://www.shortwave.com/blog/ai-email-summaries/},
	abstract = {Leverage the latest in AI to capture key points from emails in seconds},
	language = {en},
	urldate = {2023-03-01},
	journal = {Shortwave},
	author = {Wenger, Jacob},
	year = {2023},
}

@misc{mehta_shortwave_2023,
	title = {Shortwave email app introduces {AI}-powered summaries},
	url = {https://techcrunch.com/2023/03/01/shortwave-email-app-introduces-ai-powered-summaries/},
	abstract = {Shortwave email app has introduced an AI-powered summary feature so you don't have to read long emails or threads to get a gist.},
	language = {en-US},
	urldate = {2023-03-01},
	journal = {TechCrunch},
	author = {Mehta, Ivan},
	month = mar,
	year = {2023},
}

@misc{zhao_notion_2023,
	title = {Notion {AI} is here, for everyone},
	url = {https://www.notion.so/blog/notion-ai-is-here-for-everyone},
	abstract = {Discover how Notion AI is augmenting human intellect and revolutionizing the way we work},
	language = {en-us},
	urldate = {2023-03-01},
	journal = {Notion},
	author = {Zhao, Ivan},
	month = feb,
	year = {2023},
}

@misc{szyndzielorz_opera_2023,
	title = {Opera enters the generative {AI} space with new features in browsers and content apps},
	url = {https://blogs.opera.com/news/2023/02/opera-aigc-integration/},
	abstract = {Opera has today announced the upcoming integration of AI-generated content (AIGC) services into its PC and mobile browsers.},
	urldate = {2023-03-01},
	journal = {Opera News},
	author = {Szyndzielorz, Julia},
	month = feb,
	year = {2023},
}

@inproceedings{gliwa_samsum_2019,
	address = {Hong Kong, China},
	title = {{SAMSum} {Corpus}: {A} {Human}-annotated {Dialogue} {Dataset} for {Abstractive} {Summarization}},
	shorttitle = {{SAMSum} {Corpus}},
	url = {https://aclanthology.org/D19-5409},
	doi = {10.18653/v1/D19-5409},
	abstract = {This paper introduces the SAMSum Corpus, a new dataset with abstractive dialogue summaries. We investigate the challenges it poses for automated summarization by testing several models and comparing their results with those obtained on a corpus of news articles. We show that model-generated summaries of dialogues achieve higher ROUGE scores than the model-generated summaries of news – in contrast with human evaluators' judgement. This suggests that a challenging task of abstractive dialogue summarization requires dedicated models and non-standard quality measures. To our knowledge, our study is the first attempt to introduce a high-quality chat-dialogues corpus, manually annotated with abstractive summarizations, which can be used by the research community for further studies.},
	urldate = {2023-03-01},
	booktitle = {Proceedings of the 2nd {Workshop} on {New} {Frontiers} in {Summarization}},
	publisher = {Association for Computational Linguistics},
	author = {Gliwa, Bogdan and Mochol, Iwona and Biesek, Maciej and Wawer, Aleksander},
	month = nov,
	year = {2019},
	pages = {70--79},
}

@misc{nallapati_abstractive_2016,
	title = {Abstractive {Text} {Summarization} {Using} {Sequence}-to-{Sequence} {RNNs} and {Beyond}},
	url = {http://arxiv.org/abs/1602.06023},
	abstract = {In this work, we model abstractive text summarization using Attentional Encoder-Decoder Recurrent Neural Networks, and show that they achieve state-of-the-art performance on two different corpora. We propose several novel models that address critical problems in summarization that are not adequately modeled by the basic architecture, such as modeling key-words, capturing the hierarchy of sentence-to-word structure, and emitting words that are rare or unseen at training time. Our work shows that many of our proposed models contribute to further improvement in performance. We also propose a new dataset consisting of multi-sentence summaries, and establish performance benchmarks for further research.},
	urldate = {2023-03-01},
	publisher = {arXiv},
	author = {Nallapati, Ramesh and Zhou, Bowen and santos, Cicero Nogueira dos and Gulcehre, Caglar and Xiang, Bing},
	month = aug,
	year = {2016},
	note = {arXiv:1602.06023 [cs]
	version: 5},
	keywords = {Computer Science - Computation and Language},
}

@misc{iyer_opt-iml_2023,
	title = {{OPT}-{IML}: {Scaling} {Language} {Model} {Instruction} {Meta} {Learning} through the {Lens} of {Generalization}},
	shorttitle = {{OPT}-{IML}},
	url = {http://arxiv.org/abs/2212.12017},
	doi = {10.48550/arXiv.2212.12017},
	abstract = {Recent work has shown that fine-tuning large pre-trained language models on a collection of tasks described via instructions, a.k.a. instruction-tuning, improves their zero and few-shot generalization to unseen tasks. However, there is a limited understanding of the performance trade-offs of different decisions made during the instruction-tuning process. These decisions include the scale and diversity of the instruction-tuning benchmark, different task sampling strategies, fine-tuning with and without demonstrations, training using specialized datasets for reasoning and dialogue, and finally, the fine-tuning objectives themselves. In this paper, we characterize the effect of instruction-tuning decisions on downstream task performance when scaling both model and benchmark sizes. To this end, we create OPT-IML Bench: a large benchmark for Instruction Meta-Learning (IML) of 2000 NLP tasks consolidated into task categories from 8 existing benchmarks, and prepare an evaluation framework to measure three types of model generalizations: to tasks from fully held-out categories, to held-out tasks from seen categories, and to held-out instances from seen tasks. Through the lens of this framework, we first present insights about instruction-tuning decisions as applied to OPT-30B and further exploit these insights to train OPT-IML 30B and 175B, which are instruction-tuned versions of OPT. OPT-IML demonstrates all three generalization abilities at both scales on four different evaluation benchmarks with diverse tasks and input formats -- PromptSource, FLAN, Super-NaturalInstructions, and UnifiedSKG. Not only does it significantly outperform OPT on all benchmarks but is also highly competitive with existing models fine-tuned on each specific benchmark. We release OPT-IML at both scales, together with the OPT-IML Bench evaluation framework.},
	urldate = {2023-03-01},
	publisher = {arXiv},
	author = {Iyer, Srinivasan and Lin, Xi Victoria and Pasunuru, Ramakanth and Mihaylov, Todor and Simig, Daniel and Yu, Ping and Shuster, Kurt and Wang, Tianlu and Liu, Qing and Koura, Punit Singh and Li, Xian and O'Horo, Brian and Pereyra, Gabriel and Wang, Jeff and Dewan, Christopher and Celikyilmaz, Asli and Zettlemoyer, Luke and Stoyanov, Ves},
	month = jan,
	year = {2023},
	note = {arXiv:2212.12017 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{muennighoff_crosslingual_2022,
	title = {Crosslingual {Generalization} through {Multitask} {Finetuning}},
	url = {http://arxiv.org/abs/2211.01786},
	abstract = {Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are publicly available at https://github.com/bigscience-workshop/xmtf.},
	urldate = {2023-03-01},
	publisher = {arXiv},
	author = {Muennighoff, Niklas and Wang, Thomas and Sutawika, Lintang and Roberts, Adam and Biderman, Stella and Scao, Teven Le and Bari, M. Saiful and Shen, Sheng and Yong, Zheng-Xin and Schoelkopf, Hailey and Tang, Xiangru and Radev, Dragomir and Aji, Alham Fikri and Almubarak, Khalid and Albanie, Samuel and Alyafeai, Zaid and Webson, Albert and Raff, Edward and Raffel, Colin},
	month = nov,
	year = {2022},
	note = {arXiv:2211.01786 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{sanh_multitask_2022,
	title = {Multitask {Prompted} {Training} {Enables} {Zero}-{Shot} {Task} {Generalization}},
	url = {http://arxiv.org/abs/2110.08207},
	abstract = {Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks (Brown et al., 2020). It has been hypothesized that this is a consequence of implicit multitask learning in language models' pretraining (Radford et al., 2019). Can zero-shot generalization instead be directly induced by explicit multitask learning? To test this question at scale, we develop a system for easily mapping any natural language tasks into a human-readable prompted form. We convert a large set of supervised datasets, each with multiple prompts with diverse wording. These prompted datasets allow for benchmarking the ability of a model to perform completely held-out tasks. We fine-tune a pretrained encoder-decoder model (Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a wide variety of tasks. The model attains strong zero-shot performance on several standard datasets, often outperforming models up to 16x its size. Further, our approach attains strong performance on a subset of tasks from the BIG-bench benchmark, outperforming models up to 6x its size. All trained models are available at https://github.com/bigscience-workshop/t-zero and all prompts are available at https://github.com/bigscience-workshop/promptsource.},
	urldate = {2023-03-01},
	publisher = {arXiv},
	author = {Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H. and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Scao, Teven Le and Raja, Arun and Dey, Manan and Bari, M. Saiful and Xu, Canwen and Thakker, Urmish and Sharma, Shanya Sharma and Szczechla, Eliza and Kim, Taewoon and Chhablani, Gunjan and Nayak, Nihal and Datta, Debajyoti and Chang, Jonathan and Jiang, Mike Tian-Jian and Wang, Han and Manica, Matteo and Shen, Sheng and Yong, Zheng Xin and Pandey, Harshit and Bawden, Rachel and Wang, Thomas and Neeraj, Trishala and Rozen, Jos and Sharma, Abheesht and Santilli, Andrea and Fevry, Thibault and Fries, Jason Alan and Teehan, Ryan and Bers, Tali and Biderman, Stella and Gao, Leo and Wolf, Thomas and Rush, Alexander M.},
	month = mar,
	year = {2022},
	note = {arXiv:2110.08207 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{chung_scaling_2022,
	title = {Scaling {Instruction}-{Finetuned} {Language} {Models}},
	url = {http://arxiv.org/abs/2210.11416},
	abstract = {Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4\% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2\% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.},
	urldate = {2023-03-01},
	publisher = {arXiv},
	author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Castro-Ros, Alex and Pellat, Marie and Robinson, Kevin and Valter, Dasha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},
	month = dec,
	year = {2022},
	note = {arXiv:2210.11416 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{bigscience_workshop_bloom_2022,
	title = {{BLOOM}: {A} {176B}-{Parameter} {Open}-{Access} {Multilingual} {Language} {Model}},
	shorttitle = {{BLOOM}},
	url = {http://arxiv.org/abs/2211.05100},
	doi = {10.48550/arXiv.2211.05100},
	abstract = {Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.},
	urldate = {2023-02-28},
	publisher = {arXiv},
	author = {{BigScience Workshop}},
	month = dec,
	year = {2022},
	note = {arXiv:2211.05100 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{touvron_llama_2023,
	title = {{LLaMA}: {Open} and {Efficient} {Foundation} {Language} {Models}},
	abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla70B and PaLM-540B. We release all our models to the research community1.},
	language = {en},
	author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothee and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
	year = {2023},
}

@misc{hillier_introducing_2023,
	title = {Introducing {Cohere} {Summarize} {Beta}: {A} {New} {Endpoint} for {Text} {Summarization}},
	shorttitle = {Introducing {Cohere} {Summarize} {Beta}},
	url = {https://txt.cohere.ai/summarize-beta/},
	abstract = {TL;DR:
	
	“Artificial intelligence firm Cohere has launched a text summarization endpoint. Built on the firm's foundational language model, the Cohere Summarize beta service allows the condensing of essential information from large documents. The system can process long documents of up to 50,000 characters and includes a series of},
	language = {en},
	urldate = {2023-03-01},
	journal = {Context by Cohere},
	author = {Hillier, Sheena and Gallé, Matthias},
	month = feb,
	year = {2023},
}

@misc{saleh_auto-generated_2022,
	title = {Auto-generated {Summaries} in {Google} {Docs}},
	url = {https://ai.googleblog.com/2022/03/auto-generated-summaries-in-google-docs.html},
	language = {en},
	urldate = {2023-02-28},
	journal = {Google Research Blog},
	author = {Saleh, Mohammad and Kannan, Anjuli},
	month = mar,
	year = {2022},
}

@misc{saleh_conversation_2022,
	title = {Conversation {Summaries} in {Google} {Chat}},
	url = {https://ai.googleblog.com/2022/11/conversation-summaries-in-google-chat.html},
	language = {en},
	urldate = {2023-02-28},
	journal = {Google Research Blog},
	author = {Saleh, Mohammad and Wang, Yinan},
	month = nov,
	year = {2022},
}

@misc{herskowitz_microsoft_2023,
	title = {Microsoft {Teams} {Premium}: {Cut} costs and add {AI}-powered productivity},
	shorttitle = {Microsoft {Teams} {Premium}},
	url = {https://www.microsoft.com/en-us/microsoft-365/blog/2023/02/01/microsoft-teams-premium-cut-costs-and-add-ai-powered-productivity/},
	abstract = {Unlock new experiences with Microsoft Teams Premium for customers looking to make meetings more intelligent, personalized, and protected.},
	language = {en-US},
	urldate = {2023-02-28},
	journal = {Microsoft 365 Blog},
	author = {Herskowitz, Nicole},
	month = feb,
	year = {2023},
}

@misc{iyer_opt-iml_2023-1,
	title = {{OPT}-{IML}: {Scaling} {Language} {Model} {Instruction} {Meta} {Learning} through the {Lens} of {Generalization}},
	shorttitle = {{OPT}-{IML}},
	url = {http://arxiv.org/abs/2212.12017},
	doi = {10.48550/arXiv.2212.12017},
	abstract = {Recent work has shown that fine-tuning large pre-trained language models on a collection of tasks described via instructions, a.k.a. instruction-tuning, improves their zero and few-shot generalization to unseen tasks. However, there is a limited understanding of the performance trade-offs of different decisions made during the instruction-tuning process. These decisions include the scale and diversity of the instruction-tuning benchmark, different task sampling strategies, fine-tuning with and without demonstrations, training using specialized datasets for reasoning and dialogue, and finally, the fine-tuning objectives themselves. In this paper, we characterize the effect of instruction-tuning decisions on downstream task performance when scaling both model and benchmark sizes. To this end, we create OPT-IML Bench: a large benchmark for Instruction Meta-Learning (IML) of 2000 NLP tasks consolidated into task categories from 8 existing benchmarks, and prepare an evaluation framework to measure three types of model generalizations: to tasks from fully held-out categories, to held-out tasks from seen categories, and to held-out instances from seen tasks. Through the lens of this framework, we first present insights about instruction-tuning decisions as applied to OPT-30B and further exploit these insights to train OPT-IML 30B and 175B, which are instruction-tuned versions of OPT. OPT-IML demonstrates all three generalization abilities at both scales on four different evaluation benchmarks with diverse tasks and input formats -- PromptSource, FLAN, Super-NaturalInstructions, and UnifiedSKG. Not only does it significantly outperform OPT on all benchmarks but is also highly competitive with existing models fine-tuned on each specific benchmark. We release OPT-IML at both scales, together with the OPT-IML Bench evaluation framework.},
	urldate = {2023-02-28},
	publisher = {arXiv},
	author = {Iyer, Srinivasan and Lin, Xi Victoria and Pasunuru, Ramakanth and Mihaylov, Todor and Simig, Daniel and Yu, Ping and Shuster, Kurt and Wang, Tianlu and Liu, Qing and Koura, Punit Singh and Li, Xian and O'Horo, Brian and Pereyra, Gabriel and Wang, Jeff and Dewan, Christopher and Celikyilmaz, Asli and Zettlemoyer, Luke and Stoyanov, Ves},
	month = jan,
	year = {2023},
	note = {arXiv:2212.12017 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{zhang_opt_2022,
	title = {{OPT}: {Open} {Pre}-trained {Transformer} {Language} {Models}},
	shorttitle = {{OPT}},
	url = {http://arxiv.org/abs/2205.01068},
	doi = {10.48550/arXiv.2205.01068},
	abstract = {Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.},
	urldate = {2023-02-28},
	publisher = {arXiv},
	author = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Shuster, Kurt and Simig, Daniel and Koura, Punit Singh and Sridhar, Anjali and Wang, Tianlu and Zettlemoyer, Luke},
	month = jun,
	year = {2022},
	note = {arXiv:2205.01068 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{muennighoff_crosslingual_2022-1,
	title = {Crosslingual {Generalization} through {Multitask} {Finetuning}},
	url = {http://arxiv.org/abs/2211.01786},
	doi = {10.48550/arXiv.2211.01786},
	abstract = {Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are publicly available at https://github.com/bigscience-workshop/xmtf.},
	urldate = {2023-02-28},
	publisher = {arXiv},
	author = {Muennighoff, Niklas and Wang, Thomas and Sutawika, Lintang and Roberts, Adam and Biderman, Stella and Scao, Teven Le and Bari, M. Saiful and Shen, Sheng and Yong, Zheng-Xin and Schoelkopf, Hailey and Tang, Xiangru and Radev, Dragomir and Aji, Alham Fikri and Almubarak, Khalid and Albanie, Samuel and Alyafeai, Zaid and Webson, Albert and Raff, Edward and Raffel, Colin},
	month = nov,
	year = {2022},
	note = {arXiv:2211.01786 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{taylor_galactica_2022,
	title = {Galactica: {A} {Large} {Language} {Model} for {Science}},
	shorttitle = {Galactica},
	url = {http://arxiv.org/abs/2211.09085},
	doi = {10.48550/arXiv.2211.09085},
	abstract = {Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2\% versus 49.0\%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3\% to 35.7\%, and PaLM 540B on MATH with a score of 20.4\% versus 8.8\%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6\% and 52.9\%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community.},
	urldate = {2023-02-28},
	publisher = {arXiv},
	author = {Taylor, Ross and Kardas, Marcin and Cucurull, Guillem and Scialom, Thomas and Hartshorn, Anthony and Saravia, Elvis and Poulton, Andrew and Kerkez, Viktor and Stojnic, Robert},
	month = nov,
	year = {2022},
	note = {arXiv:2211.09085 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Statistics - Machine Learning},
}

@misc{hoffmann_training_2022,
	title = {Training {Compute}-{Optimal} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2203.15556},
	abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4\${\textbackslash}times\$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over Gopher.},
	urldate = {2023-02-28},
	publisher = {arXiv},
	author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and Driessche, George van den and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
	month = mar,
	year = {2022},
	note = {arXiv:2203.15556 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{rae_scaling_2022,
	title = {Scaling {Language} {Models}: {Methods}, {Analysis} \& {Insights} from {Training} {Gopher}},
	shorttitle = {Scaling {Language} {Models}},
	url = {http://arxiv.org/abs/2112.11446},
	abstract = {Language modelling provides a step towards intelligent communication systems by harnessing large repositories of written human knowledge to better predict and understand the world. In this paper, we present an analysis of Transformer-based language model performance across a wide range of model scales -- from models with tens of millions of parameters up to a 280 billion parameter model called Gopher. These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority. Gains from scale are largest in areas such as reading comprehension, fact-checking, and the identification of toxic language, but logical and mathematical reasoning see less benefit. We provide a holistic analysis of the training dataset and model's behaviour, covering the intersection of model scale with bias and toxicity. Finally we discuss the application of language models to AI safety and the mitigation of downstream harms.},
	urldate = {2023-02-28},
	publisher = {arXiv},
	author = {Rae, Jack W. and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and Rutherford, Eliza and Hennigan, Tom and Menick, Jacob and Cassirer, Albin and Powell, Richard and Driessche, George van den and Hendricks, Lisa Anne and Rauh, Maribeth and Huang, Po-Sen and Glaese, Amelia and Welbl, Johannes and Dathathri, Sumanth and Huang, Saffron and Uesato, Jonathan and Mellor, John and Higgins, Irina and Creswell, Antonia and McAleese, Nat and Wu, Amy and Elsen, Erich and Jayakumar, Siddhant and Buchatskaya, Elena and Budden, David and Sutherland, Esme and Simonyan, Karen and Paganini, Michela and Sifre, Laurent and Martens, Lena and Li, Xiang Lorraine and Kuncoro, Adhiguna and Nematzadeh, Aida and Gribovskaya, Elena and Donato, Domenic and Lazaridou, Angeliki and Mensch, Arthur and Lespiau, Jean-Baptiste and Tsimpoukelli, Maria and Grigorev, Nikolai and Fritz, Doug and Sottiaux, Thibault and Pajarskas, Mantas and Pohlen, Toby and Gong, Zhitao and Toyama, Daniel and d'Autume, Cyprien de Masson and Li, Yujia and Terzi, Tayfun and Mikulik, Vladimir and Babuschkin, Igor and Clark, Aidan and Casas, Diego de Las and Guy, Aurelia and Jones, Chris and Bradbury, James and Johnson, Matthew and Hechtman, Blake and Weidinger, Laura and Gabriel, Iason and Isaac, William and Lockhart, Ed and Osindero, Simon and Rimell, Laura and Dyer, Chris and Vinyals, Oriol and Ayoub, Kareem and Stanway, Jeff and Bennett, Lorrayne and Hassabis, Demis and Kavukcuoglu, Koray and Irving, Geoffrey},
	month = jan,
	year = {2022},
	note = {arXiv:2112.11446 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{chowdhery_palm_2022,
	title = {{PaLM}: {Scaling} {Language} {Modeling} with {Pathways}},
	shorttitle = {{PaLM}},
	url = {http://arxiv.org/abs/2204.02311},
	doi = {10.48550/arXiv.2204.02311},
	abstract = {Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.},
	urldate = {2023-02-28},
	publisher = {arXiv},
	author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
	month = oct,
	year = {2022},
	note = {arXiv:2204.02311 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{fabbri_summeval_2020,
	title = {{SummEval}: {Re}-evaluating {Summarization} {Evaluation}},
	shorttitle = {{SummEval}},
	url = {https://arxiv.org/abs/2007.12626v4},
	abstract = {The scarcity of comprehensive up-to-date studies on evaluation metrics for text summarization and the lack of consensus regarding evaluation protocols continue to inhibit progress. We address the existing shortcomings of summarization evaluation methods along five dimensions: 1) we re-evaluate 14 automatic evaluation metrics in a comprehensive and consistent fashion using neural summarization model outputs along with expert and crowd-sourced human annotations, 2) we consistently benchmark 23 recent summarization models using the aforementioned automatic evaluation metrics, 3) we assemble the largest collection of summaries generated by models trained on the CNN/DailyMail news dataset and share it in a unified format, 4) we implement and share a toolkit that provides an extensible and unified API for evaluating summarization models across a broad range of automatic metrics, 5) we assemble and share the largest and most diverse, in terms of model types, collection of human judgments of model-generated summaries on the CNN/Daily Mail dataset annotated by both expert judges and crowd-source workers. We hope that this work will help promote a more complete evaluation protocol for text summarization as well as advance research in developing evaluation metrics that better correlate with human judgments.},
	language = {en},
	urldate = {2023-02-28},
	journal = {arXiv.org},
	author = {Fabbri, Alexander R. and Kryściński, Wojciech and McCann, Bryan and Xiong, Caiming and Socher, Richard and Radev, Dragomir},
	month = jul,
	year = {2020},
	doi = {10.48550/arXiv.2007.12626},
}

@article{rael_exploring_nodate,
	title = {Exploring the {Limits} of {Transfer} {Learning} with a {Uniﬁed} {Text}-to-{Text} {Transformer}},
	language = {en},
	author = {Raﬀel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
}

@misc{zhang_ai_2022,
	title = {The {AI} {Index} 2022 {Annual} {Report}},
	url = {http://arxiv.org/abs/2205.03468},
	doi = {10.48550/arXiv.2205.03468},
	abstract = {Welcome to the fifth edition of the AI Index Report! The latest edition includes data from a broad set of academic, private, and nonprofit organizations as well as more self-collected data and original analysis than any previous editions, including an expanded technical performance chapter, a new survey of robotics researchers around the world, data on global AI legislation records in 25 countries, and a new chapter with an in-depth analysis of technical AI ethics metrics. The AI Index Report tracks, collates, distills, and visualizes data related to artificial intelligence. Its mission is to provide unbiased, rigorously vetted, and globally sourced data for policymakers, researchers, executives, journalists, and the general public to develop a more thorough and nuanced understanding of the complex field of AI. The report aims to be the world's most credible and authoritative source for data and insights about AI.},
	urldate = {2023-02-28},
	publisher = {arXiv},
	author = {Zhang, Daniel and Maslej, Nestor and Brynjolfsson, Erik and Etchemendy, John and Lyons, Terah and Manyika, James and Ngo, Helen and Niebles, Juan Carlos and Sellitto, Michael and Sakhaee, Ellie and Shoham, Yoav and Clark, Jack and Perrault, Raymond},
	month = may,
	year = {2022},
	note = {arXiv:2205.03468 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{goyal_news_2022,
	title = {News {Summarization} and {Evaluation} in the {Era} of {GPT}-3},
	url = {http://arxiv.org/abs/2209.12356},
	abstract = {The recent success of zero- and few-shot prompting with models like GPT-3 has led to a paradigm shift in NLP research. In this paper, we study its impact on text summarization, focusing on the classic benchmark domain of news summarization. First, we investigate how zero-shot GPT-3 compares against fine-tuned models trained on large summarization datasets. We show that not only do humans overwhelmingly prefer GPT-3 summaries, but these also do not suffer from common dataset-specific issues such as poor factuality. Next, we study what this means for evaluation, particularly the role of gold standard test sets. Our experiments show that both reference-based and reference-free automatic metrics, e.g. recently proposed QA- or entailment-based factuality approaches, cannot reliably evaluate zero-shot summaries. Finally, we discuss future research challenges beyond generic summarization, specifically, keyword- and aspect-based summarization, showing how dominant fine-tuning approaches compare to zero-shot prompting. To support further research, we release: (a) a corpus of 10K generated summaries from fine-tuned and zero-shot models across 4 standard summarization benchmarks, (b) 1K human preference judgments and rationales comparing different systems for generic- and keyword-based summarization.},
	urldate = {2023-02-17},
	publisher = {arXiv},
	author = {Goyal, Tanya and Li, Junyi Jessy and Durrett, Greg},
	month = sep,
	year = {2022},
	note = {arXiv:2209.12356 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{zhang_benchmarking_2023,
	title = {Benchmarking {Large} {Language} {Models} for {News} {Summarization}},
	url = {http://arxiv.org/abs/2301.13848},
	abstract = {Large language models (LLMs) have shown promise for automatic summarization but the reasons behind their successes are poorly understood. By conducting a human evaluation on ten LLMs across different pretraining methods, prompts, and model scales, we make two important observations. First, we find instruction tuning, and not model size, is the key to the LLM's zero-shot summarization capability. Second, existing studies have been limited by low-quality references, leading to underestimates of human performance and lower few-shot and finetuning performance. To better evaluate LLMs, we perform human evaluation over high-quality summaries we collect from freelance writers. Despite major stylistic differences such as the amount of paraphrasing, we find that LMM summaries are judged to be on par with human written summaries.},
	urldate = {2023-02-17},
	publisher = {arXiv},
	author = {Zhang, Tianyi and Ladhak, Faisal and Durmus, Esin and Liang, Percy and McKeown, Kathleen and Hashimoto, Tatsunori B.},
	month = jan,
	year = {2023},
	note = {arXiv:2301.13848 [cs]
	version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{el-kassas_automatic_2020,
	title = {Automatic {Text} {Summarization}: {A} {Comprehensive} {Survey}},
	volume = {165},
	shorttitle = {Automatic {Text} {Summarization}},
	doi = {10.1016/j.eswa.2020.113679},
	abstract = {Automatic Text Summarization (ATS) is becoming much more important because of the huge amount of textual content that grows exponentially on the Internet and the various archives of news articles, scientific papers, legal documents, etc. Manual text summarization consumes a lot of time, effort, cost, and even becomes impractical with the gigantic amount of textual content. Researchers have been trying to improve ATS techniques since the 1950s. ATS approaches are either extractive, abstractive, or hybrid. The extractive approach selects the most important sentences in the input document(s) then concatenates them to form the summary. The abstractive approach represents the input document(s) in an intermediate representation then generates the summary with sentences that are different than the original sentences. The hybrid approach combines both the extractive and abstractive approaches. Despite all the proposed methods, the generated summaries are still far away from the human-generated summaries. Most researches focus on the extractive approach. It is required to focus more on the abstractive and hybrid approaches. This research provides a comprehensive survey for the researchers by presenting the different aspects of ATS: approaches, methods, building blocks, techniques, datasets, evaluation methods, and future research directions.},
	journal = {Expert Systems with Applications},
	author = {El-Kassas, Wafaa and Salama, Cherif and Rafea, Ahmed and Mohamed, Hoda},
	month = jul,
	year = {2020},
	pages = {113679},
}

@misc{feng_survey_2022,
	title = {A {Survey} on {Dialogue} {Summarization}: {Recent} {Advances} and {New} {Frontiers}},
	shorttitle = {A {Survey} on {Dialogue} {Summarization}},
	url = {http://arxiv.org/abs/2107.03175},
	abstract = {Dialogue summarization aims to condense the original dialogue into a shorter version covering salient information, which is a crucial way to reduce dialogue data overload. Recently, the promising achievements in both dialogue systems and natural language generation techniques drastically lead this task to a new landscape, which results in significant research attentions. However, there still remains a lack of a comprehensive survey for this task. To this end, we take the first step and present a thorough review of this research field carefully and widely. In detail, we systematically organize the current works according to the characteristics of each domain, covering meeting, chat, email thread, customer service and medical dialogue. Additionally, we provide an overview of publicly available research datasets as well as organize two leaderboards under unified metrics. Furthermore, we discuss some future directions, including faithfulness, multi-modal, multi-domain and multi-lingual dialogue summarization, and give our thoughts respectively. We hope that this first survey of dialogue summarization can provide the community with a quick access and a general picture to this task and motivate future researches.},
	urldate = {2023-02-17},
	publisher = {arXiv},
	author = {Feng, Xiachong and Feng, Xiaocheng and Qin, Bing},
	month = apr,
	year = {2022},
	note = {arXiv:2107.03175 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{aries_automatic_2019,
	title = {Automatic text summarization: {What} has been done and what has to be done},
	shorttitle = {Automatic text summarization},
	url = {http://arxiv.org/abs/1904.00688},
	abstract = {Summaries are important when it comes to process huge amounts of information. Their most important benefit is saving time, which we do not have much nowadays. Therefore, a summary must be short, representative and readable. Generating summaries automatically can be beneficial for humans, since it can save time and help selecting relevant documents. Automatic summarization and, in particular, Automatic text summarization (ATS) is not a new research field; It was known since the 50s. Since then, researchers have been active to find the perfect summarization method. In this article, we will discuss different works in automatic summarization, especially the recent ones. We will present some problems and limits which prevent works to move forward. Most of these challenges are much more related to the nature of processed languages. These challenges are interesting for academics and developers, as a path to follow in this field.},
	urldate = {2023-02-17},
	publisher = {arXiv},
	author = {Aries, Abdelkrime and Zegour, Djamel eddine and Hidouci, Walid Khaled},
	month = apr,
	year = {2019},
	note = {arXiv:1904.00688 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{yang_exploring_2023,
	title = {Exploring the {Limits} of {ChatGPT} for {Query} or {Aspect}-based {Text} {Summarization}},
	url = {http://arxiv.org/abs/2302.08081},
	doi = {10.48550/arXiv.2302.08081},
	abstract = {Text summarization has been a crucial problem in natural language processing (NLP) for several decades. It aims to condense lengthy documents into shorter versions while retaining the most critical information. Various methods have been proposed for text summarization, including extractive and abstractive summarization. The emergence of large language models (LLMs) like GPT3 and ChatGPT has recently created significant interest in using these models for text summarization tasks. Recent studies {\textbackslash}cite\{goyal2022news, zhang2023benchmarking\} have shown that LLMs-generated news summaries are already on par with humans. However, the performance of LLMs for more practical applications like aspect or query-based summaries is underexplored. To fill this gap, we conducted an evaluation of ChatGPT's performance on four widely used benchmark datasets, encompassing diverse summaries from Reddit posts, news articles, dialogue meetings, and stories. Our experiments reveal that ChatGPT's performance is comparable to traditional fine-tuning methods in terms of Rouge scores. Moreover, we highlight some unique differences between ChatGPT-generated summaries and human references, providing valuable insights into the superpower of ChatGPT for diverse text summarization tasks. Our findings call for new directions in this area, and we plan to conduct further research to systematically examine the characteristics of ChatGPT-generated summaries through extensive human evaluation.},
	urldate = {2023-02-17},
	publisher = {arXiv},
	author = {Yang, Xianjun and Li, Yan and Zhang, Xinlu and Chen, Haifeng and Cheng, Wei},
	month = feb,
	year = {2023},
	note = {arXiv:2302.08081 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}
@misc{retkowski2023current,
	title={The Current State of Summarization (arXiv 2305.04853)}, 
	author={Fabian Retkowski},
	year={2023},
	eprint={2305.04853},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}
